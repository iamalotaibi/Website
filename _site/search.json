[
  
    {

      "title"    : "How I managed my time effectively",
      "url"      : "/posts/test",
      "content"  : "Rules\n\nI try to follow these rules:\n\nTime-blocks\n\n\n  Session (50/10): I typically work/study for 50 minutes where I work on one thing for the entire time. There isn't anything important in life besides this one thing. If life interrupt me, I will try to stop the interuption and try to get back to my zone at soon as possible. After the completion of the 50 minutes, I will take a break for 10 minutes where I will be standing and walking the majority of the time. I usually do life tasks (e.g. washing dishes, making coffee, throw trashes, etc.) then I will check my phone for messages real quick without checking any of my socials.\n  Block (6/1): Every block has 6 sessions and 1 long break. For the sessions, I can have combine two sessions if I am excited in continue what I work on and I feel that I am close by of completion and I know for sure that I have some gas to continue the same quality of work if I took break.\n\n\nTypical day\n\n\n  5:00-6:00am: Morning Routine\n  6:00-12:00pm: Deep Work 1\n  12:00-1:00pm: Lunch (break)\n  1:00-7:00pm: Deep Work 2\n  7:00-9:00pm: Night Routine"

    },
  
  
    {

      "title"    : "Discriminative Appearance Modeling with Multi-track Pooling for Real-time Multi-object Tracking",
      "url"      : "/publications/gabor",
      "content"  : "Abstract"

    },
  
  
    {

      "title"    : "Exploring Robot",
      "url"      : "/projects/exploring-robot",
      "content"  : "Instructions\n\n\n  Code A*\n  Download world.csvPreview the document\n  Write a program to create a 4-connected graph and run an A* search from vertex (0,0) to vertex (19,19) across the obstacle map provided in world.csv.\n  The world is a 20×20 grid of cells\n  The world.csv file is an occupancy grid map: 1 means the grid cell is occupied and you can’t move through it\n  Edge costs are 1\n  Your code should output the final path (either plot it or print out the vertex coordinates) and associated path cost.\n  Comment your code to demonstrate that you understand the algorithm.\n  What to turn in:\n    \n      A zip file of your commented A* code including world.csv.\n      A cover sheet (PDF) listing:\n      Web sites you used\n      People you worked with\n      The final path\n      Your heuristic function (in English)\n      How you implemented the graph and priority queue\n      Any known bugs/issues\n    \n  \n\n\nA few notes:\n\n\n  4-connected means that you can travel from a cell to any of the cardinal neighbors (north, south, east, west).\n  Broadly speaking, there are two ways you can represent the graph\n  As an adjacency matrix with a function that returns valid neighbors for a given vertex when queried, or\n  As a list of vertices and a list of edges.\n  You need to demonstrate that you understand how the algorithm works and the best way to do this is to comment relevant lines of code. Marks will be awarded accordingly.\n  There are plenty of resources are available to you online, you may take inspiration from existing implementations that you find, but see Note 3 above.\n\n\nRubric and Grade\n\n\n  \n    \n      Criteria\n      Ratings\n       \n      Pts\n    \n  \n  \n    \n      Sends waypoints to the robot\n      3.0 pts   Full Marks\n      0.0  pts  No Marks\n      3.0\n    \n    \n      Uses SLAM to create a map\n      3.0  pts  Full Marks\n      0.0   pts   No Marks\n      3.0\n    \n    \n      Has a documented exploration strategy\n      7.0    pts    Full Marks\n      0.0   pts   No Marks\n      7.0\n    \n    \n      Strategy works in all world files (entire space visited)\n      7.0    pts    Full Marks\n      0.0   pts   No Marks\n      7.0\n    \n    \n      Mechanism for detecting unexplored area\n      5.0  pts  Full Marks\n      0.0   pts   No Marks\n      5.0\n    \n    \n      Mechanism for detecting when exploration strategy fails  Failure case: didn't get to way point, what do you do next?\n      7.0   pts   Full Marks\n      0.0   pts   No Marks\n      7.0\n    \n    \n      Algorithm/strategy for getting to unexplored area\n      3.0   pts   Full Marks\n      0.0   pts   No Marks\n      3.0\n    \n    \n      Behaves \"reasonably\" on other test worlds  Doesn't crash, makes some attempt to navigate to unexplored areas\n      5.0  pts  Full Marks\n      0.0   pts   No Marks\n      5.0\n    \n    \n      Total Points:\n       \n       \n      40.0\n    \n  \n\n\n\n\nReport\n\nDiscussion of the exploration problem\n\nIn this problem we are designing an exploration package utilizing the gmapping and nav_bundle packages to allow a simulated robot to explore an unknown environment. Our algorithm will have to set waypoints to move our robot towards the unexplored areas while avoiding obstacles. The robot should be reasonably robust to noisy odometry and mapping data, and it should be able to recognize when waypoints cannot be reached.\n\nDiscussion of your gmapping and nav_bundle package implementations\n\nFrom the gmapping bundle we are only using the occupancy grid. This grid is used to find \"frontier\" points (points between explored and unexplored areas). We are also reading the map meta data which gives us the resolution of the occupancy grid in meters/pixel. This gives us the ability to transform the occupancy grid data into Cartesian coordinates. The map is also saved when a waypoint is generated and used to verify that the robot is staying in known areas only, ensuring that the robot doesn't run into walls even if it didn't know about them before it calculated it's path.\n\nFrom the nav_bundle package, we are using the waypoint commands: twist, base_link_goal, path_reset, move_base_cancel, and ready_pub.  Clear and cancel are used to have the robot only pursue a single waypoint at a time.  Waypoints generated using the occupancy grid which are then translated and rotated into the robot's local coordinate system then set as a waypoint using Twist.\n\nDiscussion of your waypoint allocation algorithm\n\nWaypoints are generated procedurally using an 61x61 filter that scans the frontier points on the occupancy grid. The robot's exploration policy defines frontier points as being known unoccupied locations where the robot would end near unknown locations. This filter only selects points that are centered on an explored point, have no obstacles within a specified distance from the center, have a threshold percentage of unexplored cells, and aren't where the robot has been before. These cells are then weighted by the percentage of unknown cells and euclidean distance from the robot. It then uses an A* algorithm to determine if there is a known path from the current location to the candidate location. Validating the path allows the robot to exclude candidate points that would be outside of the map or within obstacles.\n\nIf the robot enters a region that was unexplored when the waypoint was created, it will clear the waypoint queue, cancel the current waypoint, turn 360 degrees, back up 1.5m, and generate a new waypoint. With the current implementation of the nav_bundle, the robot can select paths that pass through unknown locations. If the robot then passes though the unknown location and discovers that there is a wall blocking the path, the robot will not reroute in order to find the proper path. Instead, the robot will simply crash into the wall. To prevent incorrect path planning through unknown locations, we save how the map looked when the nav_bundle chose that path, and tell the robot to stop and reroute if we reach a location that was previously unknown, making our robot's path robust to new information.\n\n\n    \n\\begin{algorithm}\n\\caption{FindNextWaypoint}\n\\begin{algorithmic}\n\\Procedure{FindNextWaypoint}{$map$, $window\\_size$, $stride\\_length$, $visited\\_locations$, $robot\\_loc$, $avoidance\\_radius$, $empty\\_radius$, $RESOLUTION$}\n\n\\State $potential\\_candidates \\gets \\text{[]}$\n\n\\For{\\textbf{every} $center\\_cell \\textbf{ in } map \\textbf{ that is greater than } stride\\_length$ \\textbf{apart from each other}}\n    % if the center and the surrounding cells are not empty:\n    %   continue\n    \\State $skip \\gets false$\n    \\For{\\textbf{every} $neighbor\\_cell$\\textbf{ in a }$avoidance\\_radius$\\textbf{ away from }$center\\_cell$}\n        \\If {alreadyVisited($visited\\_location$, $neighbor\\_cell$)}\n            \\State $skip \\gets true$\n            \\State \\textbf{break}\n        \\EndIf\n    \\EndFor\n    \\If {$skip == true$}\n        \\State \\textbf{continue}\n    \\EndIf\n    \\For{\\textbf{every} $neighbor\\_cell$\\textbf{ in a }$empty\\_radius$\\textbf{ away from }$center\\_cell$}\n        \\If {getMapValue($visited\\_location$) $\\neq$ $0$}\n            \\State $skip \\gets true$\n            \\State \\textbf{break}\n        \\EndIf\n    \\EndFor\n    \\If {$skip == true$}\n        \\State \\textbf{continue}\n    \\EndIf\n    \n    \\State $cell\\_sum \\gets \\text{sum(all cells within } window\\_size/2 \\text{ from } center\\_cell \\text{)}$\n\t\n\t\\State $potential\\_candidates\\text{.append(}center\\_cell\\text{)}$\n\\EndFor\n\n\\State $candidate \\gets \\text{sorted(}potential\\_candidates\\text{)} $\n\\State $candidate \\gets  potential\\_candidates\\text{.pop()} $\n\\While{$\\textbf{not } \\text{reachableByAStar(} robot\\_loc, candidate\\text{)}$}\n    \\State $candidate \\gets  potential\\_candidates\\text{.pop()} $\n    \\State $best\\_loc=candidate$\n\\EndWhile\n\n\\State $candidate  \\gets \\text{convert\\_row\\_col\\_to\\_coord}best\\_loc, RESOLUTION, map\\text{)} $\n\n\\State \\textbf{return} $candidate$\n\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\n\n\n  Inputs:\n    \n      map: The current occupancy grid\n      window_size: Width of the sliding window used to generate candidate points\n      stride_length: Row much to shift the sliding window with each iteration\n      visited_locations: Grid with same size as map representing the locations we have been to\n      robot_loc: Currently location of the robot\n      avoidance_radius: Radius around candidate waypoint that should not contain walls\n      empty_radius: Radius around candidate waypoint that should be empty\n      RESOLUTION: How many meters per cell\n    \n  \n  Outputs:\n    \n      candidate: The best potential waypoint to go to next\n    \n  \n\n\nAnalysis of your algorithm’s exploration performance\n\nThe algorithm has managed full coverage in all provided maps. This algorithm is far from perfect, periodically getting stuck (although it is able to correct itself), and periodically choosing waypoints outside of the map. The algorithm can struggle to come up with waypoints in a reasonable amount of time (typically 20 seconds) due to the need to run A* after a potential candidate is selected.\n\nThe algorithm tended to get the robot stuck in one area, ensuring that every cell in the immediate area. This behavior causes the exploration process to take a long time in the maze map if it selects a point on the other side of a wall potentially causing long travel times.\n\nDid it result in full coverage of the environment (provide a screenshot from rviz)?\n\n\n    \n    \n    \n\n\n\tFigure 1: Random Dots Map, The Office Map, and The Maze Map\n\n\nThis algorithm resulted in full exploration of the robot's environment with a reasonable success rate. The many dots map was the first map that the robot completed, it completed the map in around 20 minutes. Euclidean distance in this map is an accurate way to approximate path distance since the map isn't divided into rooms, making it an ideal scenario for this algorithm. The office map saw completion times of around 22 minutes. This was largely due to the euclidean distance heuristic that had the robot fully explore each room before moving on. This algorithm struggled with the maze file since it had a habit of setting waypoint on the other side of a wall making it drive back and forth around the entire map, making little but steady progress.\n\nProvide suggestions on how your waypoint allocation algorithm could be improved.\n\nThe algorithm should use an A* search in order to find the nearest waypoints instead of using euclidean distance. This should make the exploration time faster since it would travel through fully explored areas less often in order to reach new locations. Computationally expensive functions (such as A*) could be rewritten in C++ in order to cut down on computational cost. The robot's policy for getting unstuck could also be improved. Currently we assume that the robot gets stuck, it is facing a wall and can therefor get unstuck by backing up enough. However, it is not always the case that the wall is in front of the robot, and backing up could end up moving the robot into a wall. Instead, a better policy would be to turn and move away from the closest wall whenever the robot is stuck.\n\nSetup\n\n  Download the file (source code)\n  Unzip the file into the \\catkin_ws\\src directory\n  In \\catkin_ws, run this command to build\n\n\n1\ncatkin_make\n\n\nRun\n\n  Open a new terminal\n  In \\catkin_ws, run this command to source the bash file\n\n\n1\nsource devel/setup.bash\n\n\n\n  Run this command to launch the ros package\n\n\n1\nroslaunch src/rob456_project/launch/rob456_project.launch\n\n\n\n  Open a new terminal\n  In \\catkin_ws, run this command to source the bash file\n\n\n1\nsource devel/setup.bash\n\n\n\n  Run this command to launch the ros package\n\n\n1\nroslaunch src/nav_bundle/launch/nav_bundle.launch\n\n\n\n  When the world starts, in rviz, select '2D Nav Goal' and point to a closer cell that has been explored\n\n\nContributors\n\n\n  Lucas Frey lcsfrey\n  Mazen Alotaibi sudomaze\n  Daniel Boreham"

    },
  
    {

      "title"    : "Linux vs. FreeBSD vs. Windows",
      "url"      : "/projects/linux-vs-freebsd-vs-windows",
      "content"  : "Introduction\n\nThis paper presents a comparison between Linux, FreeBSD, and Windows in three areas: concurrency, I/O, and memory management. In addition, in each area, I will discussing about the differences and similarities between them.\n\nConcurrency Comparison\n\nIn this section, I will be discussing about processes, threads, CPU scheduling, and other related information that I have found interesting for each operating system based on my research.\n\nLinux\n\nEach process provides the resources needed to execute a program. A process has a virtual address space, executable code, open handles to system objects, a security context, a unique process identifier, environment variables, a priority class, minimum and maximum working set sizes, and at least one thread of execution. Each process is started with a single thread, often called the primary thread, but can create additional threads from any of its threads [1].\n\nTo create a process in Linux, we will need to use Fork() system call, which creates a new process, called the child process, from the exiting process, called the parent process. The child process has its own process ID (PID). Fork() takes no argument and return process ID. Fork() returns negative value if the process isn't created, zero if the child process is created, and positive value and a child process ID if Fork() returns from parent process. In addition, system call Fork() duplicate the same address space of the parent process and allocate to the child process. However, the child process doesn't inherit timer and semaphore adjustment from the parent process [2].\n\nThe following code sample demonstrates how to create a process in Linux [3]:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n#include &lt;unistd.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;errno.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;sys/wait.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main( )\n{\n    pid_t child_pid;\n    child_pid = fork ( );                                    // Create a new child process;\n    if (child_pid &gt;= 0)                         \n    {\n        if (child_pid == 0)\n        {\n            printf (\"child process successfully created!!\\n\");\n            printf (\"child PID =  %d, parent PID = %d\\n\", getpid( ), getppid( ) );\n            exit(0);\n         }\n    }\n    else\n    {\n        perror(\"fork\");\n        exit(0);\n    }\n}\n\n\nThreads of execution, often shortened to threads, are the objects of activity within the process. Each thread includes a unique program counter, process stack, and set of processor registers. The kernel schedules individual threads, not processes. In traditional Unix systems, each process consists of one thread. In modern systems, however, multi-threaded programs consist of more than one thread are common [1].\n\nFreeBSD\n\nA process is a program in execution. Each process has an address space containing a mapping of its program’s object code and global variables, a set of kernel resources that it can name and on which it can operate using system calls, and at least one and possibly many threads that execute its code. Every process in the system is assigned a unique identifier termed the process identifier (PID). An PID is a common mechanism used by applications and by the kernel to reference processes and it is used by applications when the latter send a signal to a process and when receiving the exit status from a deceased process. There are two PIDs that are special important to to each process:  the PID of the process itself and the PID of the process’s parent process. A process structure contains information that must always remain resident in main memory, along with references to other structures that remain resident [4].\n\nEvery thread represents a virtual processor with a full context worth of register state and its own stack mapped into the address space. In addition, every thread running in the process has a corresponding kernel thread, with its own kernel stack that represents the user thread when it is executing in the kernel as a result of a system call, page fault, or signal delivery. The threads of a process operate in either user mode or kernel mode. In user mode, a thread executes application code with the machine in a non-privileged protection mode. Thread structure tracks information that needs to be resident only when the process is executing such as its kernel run-time stack. Both, process and thread, structures are allocated dynamically as part of process creation and are freed when the process is destroyed as it exits [4].\n\nThe FreeBSD timeshare scheduler uses a priority-based scheduling policy that is biased to favor interactive programs, such as text editors, over long-running batch-type jobs. Interactive programs tend to exhibit short bursts of computation followed by periods of inactivity or I/O. The scheduling policy initially assigns a high execution priority to each thread and allows that thread to execute for a fixed time slice [4].\n\nWindows\n\nA process is basically a program in execution. The execution of a process must progress in a sequential fashion. Each process is uniquely identified by a number called a process ID (PID). Similar to files, each process has one owner and group, and the owner and group permissions are used to determine which files and devices the process can open [5].\n\nIn addition, to create a process in Windows, we will need to use CreateProcess function. CreateProcess function runs independently of the creating process, and the following code sample demonstrates how to create a process [5]:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n#include &lt;windows.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;tchar.h&gt;\n\nvoid _tmain( int argc, TCHAR *argv[] )\n{\n    STARTUPINFO si;\n    PROCESS_INFORMATION pi;\n    ZeroMemory( &amp;si, sizeof(si) );\n    si.cb = sizeof(si);\n    ZeroMemory( &amp;pi, sizeof(pi) );\n    if( argc != 2 )\n    {\n        printf(\"Usage: %s [cmdline]\\n\", argv[0]);\n        return;\n    }\n    // Start the child process. \n    if( !CreateProcess( NULL,   // No module name (use command line)\n        argv[1],        // Command line\n        NULL,           // Process handle not inheritable\n        NULL,           // Thread handle not inheritable\n        FALSE,          // Set handle inheritance to FALSE\n        0,              // No creation flags\n        NULL,           // Use parent's environment block\n        NULL,           // Use parent's starting directory\n        &amp;si,            // Pointer to STARTUPINFO structure\n        &amp;pi )           // Pointer to PROCESS_INFORMATION structure\n    ) \n    {\n        printf( \"CreateProcess failed (%d).\\n\", GetLastError() );\n        return;\n    }\n    // Wait until child process exits.\n    WaitForSingleObject( pi.hProcess, INFINITE );\n    // Close process and thread handles.\n    CloseHandle( pi.hProcess );\n    CloseHandle( pi.hThread );\n}\n\n\nWindows implements a priority-driven, preemptive scheduling system, at least one of the highest priority ready threads always runs, with the caution that certain high-priority threads ready to run might be limited by the processors on which they might be allowed or preferred to run on, a phenomenon called processor affinity [6].\n\nGeneral Discussion\n\nFrom my research, they all have similar processes features and behaviors, and Windows' threads are similar to FreeBSD in the current model and the interface definition for Windows' threads are similar to Linux. In addition, FreeBSD and Windows have a similar CPU scheduling as they both use priority-queues. [1][2][4][5][6].\n\nIO Comparison\n\nIn this section, I will be discussing about I/O (block and character), data structures, algorithms, cryptography, I/O scheduling, types of devices, and other related information that I have found interesting for each operating system based on my research.\n\nFreeBSD\n\nThe basic model of the UNIX I/O system is a sequence of bytes that can be accessed either randomly or sequentially, and there aren't any access methods and control blocks in typical UNIX user process. Different programs expect various levels of structure, but the kernel doesn't impose structure on I/O. In addition, UNIX processes use a descriptor to reference I/O streams. Descriptors are small unsigned integers obtained from the open and socket system calls. Read and write system calls can be applied to descriptors to transfer data, and close system call can be used to dead-locate any descriptor. There are three types of descriptors, files, pipes, and sockets. Files are linear array bytes, has at least one name, exists until all its names are deleted explicitly, and no process holds a descriptor for it. Pipes are a linear array of bytes, no name, used as an I/O stream, it is unidirectional, and created by a pipe system call. Sockets are transient objects, used for interprocess communication, exists only as long as some process holds a descriptor referring to it, and created by a socket system call. Furthermore, hardware devices can be categorized as either block (structure) or character (unstructured). For block devices, they are typified by disks and magnetic tapes, the kernel supports read-modify-write-type buffering actions on block-oriented structured devices to allow latter to be read and written in a totally random byte addressed fashion, and the filesystems are created on block devices. For character devices, they are communication lines, raster plotters, and unbuffered magnetic tapes and disks, and are support large block I/O transfers [7].\n\nFor previous editions of stream I/O system, the stream I/O system was based on the UNIX character I/O system, which allows a user process to open a way terminal port and then to insert appropriated kernel-processing modules and the modules that are being processed by the network protocols can be inserted in the appreciated kernel-processing modules. In addition, stacking a terminal-processing module on top of a network-processing module allowed flexible and efficient implementation of the network viral terminals within the kernel. In newer editions of stream I/O system ,such as 18th edition which was adopted in System V. However, the design of the networking facilities for 4.2BSD changed its approach based on the socket interface and flexible multi-player network architecture, which allows a single system to support multiple sets of networking protocols with stream, datagram, and other types of access. In addition, the user application and the kernel operate independently of each other for security. In 4.4BSD, the kernel doesn't store I/O control blocks or other operating-system-related data structures in the application address space. Each user-level application is provided with an independent address space in which it executes its applications/processes. Moreover, the kernel makes most of the state changes invisible to the processes involved [7].\n\nFreeBSD supports two different disk encryption methods, GBDE and GELI, and both of the methods support different cryptographic algorithms that counter different threats. For GBDE, it is high-security (protecting the user as protecting the data), cryptographic key provided by the user, and when the key is lost, the data can't be accessed. On the other hand, GELI protects the data but doesn't protect the user, it uses FreeBSD's cryptographic device driver, and takes advantage of its transparently [8].\n\nWindows\n\nThe design goals for the Windows I/O system are to provide an abstraction of devices, both hardware and software to application with a selected features of the operating system, such as uniform security, high-performance asynchronous, high-level language support, etc. Windows I/O system is responsible for the connection between user model functionality, storage, and drivers with WDM WMI Routines, PnP Manager, Power Manager, and I/O Manager. For I/O Manager, it is the core of the Windows I/O system because it defines the order of the framework within which I/O requests are delivered to device drivers. Most I/O requests are represented as I/O Request Packet (IRP), which travels from one I/O system component to another. The design of Windows I/O system allows an individual application thread to manage multiple I/O requests concurrently [9].\n\nMoreover, I have found 4 interesting algorithms built within the driver structure, Initialization Routine, Opening Devices, IRP, and Completing an I/O Request. For the Initialization Routine, the I/O manager executes a driver's Initialization Routine when it loads the driver into the operating system. Then, the Initialization Routine fills in the system data structures to register the rest of the driver's routines with the I/O manager and performs any global driver initialization that is necessary. For the Opening Devices, a file object is in a kernel model data structure that represents a handle to a device, and this process allows synchronization and easy manipulation of the object files. For IRP, the IRP is where the I/O system stores information it needs to process an I/O request, so when a thread calls an I/O API, the I/O manager constructs an IRP to represent the operation as it progresses through the I/O system. For Completing an I/O Request, it starts when a driver calls IoCompleteRequest to inform the I/O manager that has completed process teh request specified in the IRP [9].\n\nI have found a good example[10] of how to use IoCompleteRequest in a Windows machine to implement Completing an I/O Request.\n\n1\n2\n3\n4\n5\n6\n7\nNTSTATUS CompleteRequest(PIRP Irp, NTSTATUS status, ULONG_PTR Information)\n{\n    Irp-&gt;IoStatus.Status = status;\n    Irp-&gt;IoStatus.Information = Information;\n    IoCompleteRequest(Irp, IO_NO_INCREMENT);\n    return status;\n}\n\n\nGeneral Discussion\n\nA comparison between FreeBSD, Linux, and Windows 2000, that FreeBSD and Linux have higher security compared to Windows 2000 because they are both open source and Windows 2000 is closed, which means the main developers need to detect errors in the system by themselve. However, FreeBSD has higher security than Linux beucase FreeBSD requires third parties verification of the system compared to Linux, which can accepts updates from anyone with minor verification. In addition, Windows 2000 is supported by most of device manufactures because its layered architecture and generic use of objects [11][12].\n\nTo sum up, it seems that FreeBSD has a Linux-like I/O system strcture compared to Windows which treats every file as an object. In addition, FreeBSD has higher security than Linux and Windows because FreeBSD has different methods to protect data. However, Windows is supported by most of device manufactures because its layered architecture and generic use of objects [7][8][9][11][12].\n\nMemory Management Comparison\n\nIn this section, I will be discussing about memory management in FreeBSD and Windows based on my research.\n\nFreeBSD\n\nBerkeley Software Distribution (BSD) kernel handles process scheduling, memory management, symmetric multi-processing, device drivers, etc. In addition, for memory management in general, each process has its own private address and each address space is divided into 3 logical segments: text, data, and stack. For the text segment, it is read-only, has initialized and uninitialized data portions of a program, and, in most machines, a process can change the size of its text segment only when the segment's contents are overlaid with data from the files system or when debugging in action. For the stack, it has application's run-time stack and makes a system call in most machines. Lastly, initial contents of the segments of a child process are duplicated from the segments of a parent process. Moreover, for memory management inside the kernel, kernel often does allocation of memory that are needed for only the duration of a single system call, but in a user process, such as short-term memory, would be allocated on the run-time stack. Kernel's memory isn't feasible to allocate even moderate-sized blocks of memory on it because the kernel has a limited run-time stack [7].\n\nFor memory management, kernel can't easily deal with memory allocation errors and often can't scheme. Therefore, getting the memory in the kernel is more complicated than in user-space. Moreover, kernel treats phsical pages as the basic unit of memory management and kernel represents every phsical page on the system with a struct page structure, which is defined in &lt;linux/mm_types.h&gt; [1].\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nstruct page {\n    unsigned long flags;\n    atomic_t _count;\n    atomic_t _mapcount;\n    unsigned long private;\n    struct address_space *mapping;\n    pgoff_t index;\n    struct list_head lru;\n    void *virtual;\n};\n\n\nThe kernel can't treat all pages as the same because the hardware limitation. Therefore, some pages can't be used for certain because of their physical address in memory. Hence, kernel uses the zones to group pages of similar properties. Linux partitions the system's pages into zones to have a pooling in place to satisfy allocations as needed. Although some allocations may require pages from a partiular zone, other allocations my pull from multiple zones. Each zone is represented by a struct zone, which is defined in &lt;linux/mmzone.h&gt; [1].\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nstruct zone {\n    unsigned long watermark[NR_WMARK];\n    unsigned long lowmem_reserve[MAX_NR_ZONES];\n    struct per_cpu_pageset pageset[NR_CPUS];\n    spinlock_t lock;\n    struct free_area free_area[MAX_ORDER]\n    spinlock_t lru_lock;\n    struct zone_lru {\n        struct list_head list;\n        unsigned long nr_saved_scan;\n    } lru[NR_LRU_LISTS];\n    struct zone_reclaim_stat reclaim_stat;\n    unsigned long pages_scanned;\n    unsigned long flags;\n    atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];\n    int prev_priority;\n    unsigned int inactive_ratio;\n    wait_queue_head_t *wait_table;\n    unsigned long wait_table_hash_nr_entries;\n    unsigned long wait_table_bits;\n    struct pglist_data *zone_pgdat;\n    unsigned long zone_start_pfn;\n    unsigned long spanned_pages;\n    unsigned long present_pages;\n    const char *name;\n};\n\n\nKmalloc() is similar to malloc() in user-space, but it is just a simple interface for obtaining kernel memory in byte-sized chunks and it can be used to allocate pages. For freeing pages, we can use kfree(), which is definded in &lt;linux/slab.h&gt;, kfree() frees a block of memory previously allocated with kmalloc() [1].\n\n1\n2\n3\n4\n5\n    buf = kmalloc(BUF_SIZE, GFP_ATOMIC);\n    if (!buf)\n    /* error allocating memory ! */\n    ...\n    kfree(buf);\n\n\nFree lists data structures are the default data structures in kernel. Due to the fact that allocating and freeing data structures is one of the most common operations inside any kernel has issues, slab layer is used to solve these issues. Slab layer acts as generic data structure-chaining layer and slab layer attempts to cache frequently used data structures as they tend to be allocated and freed often, prevent memory fragmentation, which is resulted from frequent allocation and deallocation, by cached free lists are arranged contiguously, and it has many other promises that slab layer attempts to provide [1].\n\nWindows\n\nThe memory manager in Windows implements virtual memory, provides a core set of services such as memory mapped files, copy-on-write memory, large memory support, and underlying support for the cache manager.The memory manager creates the two memory pools, non-paged pool and paged pool, that the system uses to allocate memory. Non-paged pool and paged pool are located in the region of the address space that is reserved for the system and mapped into the virtual address space of each process. For the non-paged pool, it consists of virtual memory addresses that are guaranteed to reside in physical memory as long as the corresponding kernel objects are allocated. For the paged pool, it consists of virtual memory that can be paged in and out of the system. To improve performance, systems with a single processor have three paged pools, and multiprocessor systems have five paged pools [5].\n\nIt seems that Windows uses VirtualAlloc() to use a page granularity, so using VirtualAlloc can result in higher memory usage and it allows you to specify additional options for memory allocation. In order to free pages, you need to use VirtualFree() [5].\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nLPVOID WINAPI VirtualAlloc(\n      _In_opt_ LPVOID lpAddress,\n      _In_     SIZE_T dwSize,\n      _In_     DWORD  flAllocationType,\n      _In_     DWORD  flProtect\n);\nBOOL WINAPI VirtualFree(\n      _In_ LPVOID lpAddress,\n      _In_ SIZE_T dwSize,\n      _In_ DWORD  dwFreeType\n);\n\n\nGeneral Discussion\n\nWindows and Linux has different memory management structures. In Windows, memory management uses tree data structures, uses cluster demand paging, brings 8 pages in memory simultaneously, and page replacements uses First In First Out (FIFO) algorithm. In the other hand, in Linux, memory management uses linked lists data stricture, uses demand paging with no pre-paging and it doesn't swap the entire process instead it uses a lazy swapper, swaps the necessary pages into memory only to avoid reading pages that won't be used, which decreases swap time and amount of physical memory required, and page replacement uses Least Recently Used (LRU) algorithm [13].\n\nFrom my research, it seems that Windows is built for commercial use due it's complex systems to handle large amount of data compared to linux and FreeBSD, and FreeBSD seems to follow most of Linux memory management style, but it is more stable than Linux by using different methods [1][5][7][13].\n\nConclusion\n\nTo conclude, I have discussed about the differences and similarity between Linux, FreeBSD, and Windows in these areas: concurrency, I/O, and memory management. Therefore, based on my research and understanding, I conclude that FreeBSD and Linux are an excellent operating systems because of their system simplicity compared to Windows. However, Windows complexity was made for a certain purpose, which is commercial use. In addition, FreeBSD seems to be more stable and secure than Linux. Other than that, all of the systems have similar general architecture and data flow, but they vary in the way they implement and process these parts.\n\nReferences\n\nR. Love,Linux Kernel Development, 3rd ed. Addison-Wesley Professional, 2010.\n\n\"Operating system-processes,\"Available at https://www.tutorialspoint.com/operatingsystem/osprocesses.htm(2018/10/19).\n\nA. Vara, \"How to create process in linux (part 10/15),\" Available at https://www.engineersgarage.com/tutorials/introduction-linux-part-1015.\n\nM. K. McKusick, G. Neville-Neil, and R. N. Watson,The Design and Implementation of the FreeBSD Operating System, 2nd ed. Addison-Wesley Professional, 2014.\n\n\"Memory management,\" Available at https://docs.microsoft.com/en-us/windows/desktop/memory/memory-management(2018/05/30).\n\nM. E. Russinovich, D. A. Solomon, and A. Ionescu,Windows Internals, Part 1: Covering Windows Server 2008 R2 and Windows 7,6th ed. Redmond, WA, USA: Microsoft Press, 2012.\n\nM. K. McKusick, K. Bostic, M. J. Karels, and J. S. Quarterman,The Design and Implementation of the 4.4BSD Operating System.Redwood City, CA, USA: Addison Wesley Longman Publishing Co., Inc., 1996.\n\nM. W. Lucas,Absolute Freebsd, 2Nd Edition, 2nd ed. San Francisco, CA, USA: No Starch Press, 2007.\n\nM. E. Russinovich, D. A. Solomon, and A. Ionescu,Windows Internals, Part 2: Covering Windows Server 2008 R2 and Windows 7(Windows Internals). Redmond, WA, USA: Microsoft Press, 2012.\n\nH. Haftmann, \"Completing i/o requests,\" Available at https://www-user.tu-chemnitz.de/∼heha/oneywdm/ch05d.htm.\n\nB. Bruce and M. Stokely, \"Freebsd vs. linux vs. windows 2000,\" Available at https://people.freebsd.org/∼murray/bsdflier.html.\n\nS. Hand, \"Operating systems,\" Available at https://www.cl.cam.ac.uk/teaching/1011/OpSystems/os1a-slides.pdf\n\nU. Essays, \"Compare the memory management of windows with linux,\" Available at https://www.ukessays.com/essays/engineering/compare-the-memory-management.php (2016/12/05)."

    },
  
    {

      "title"    : "Travelling Salesman Problem",
      "url"      : "/projects/travelling-salesman-problem",
      "content"  : "Description\n\nThis is a final report for analysis of solving the travelling salesman problem using multiple algorithms and the analysis of those algorithms.\n\nAlgorithms that Solve TSP\n\nBrute Force Search (Naive Algorithm)\n\nRun-time Analysis\n\n\\[O(n!)\\]\n\nPseudo-code\n\n\n    \n\\begin{algorithm}\n\\caption{Brute Force Search}\n\\begin{algorithmic}\n\\Procedure{BF}{$r$, $cititiesNotInRoute[1...n]$}\n    \\If {$citiesNotInRoute.length != 0$}\n        \\For{$i$ \\textbf{from} $0$ \\textbf{to} $citiesNotInRoute.length$}\n            \\State $cityRemoved = popFront(cititiesNotInRoute)$\n            \\State $newRoute = r$\n            \\State $push(newRoute, cityRemoved)$\n            \\State $BF(newRoute, cititiesNotInRoute)$\n            \\State $push(cititiesNotInRoute, cityRemoved)$\n        \\EndFor\n    \\EndIf\n    \\If {$skip == true$}\n        \\State $print(r)$\n    \\EndIf\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\nDescription\n\nThis algorithm checks every vertices and every edges. Therefore, it is clear it will find the solution. The running time is $O(n!)$ because the starting vertex has $n-1$  edges to choose, next one has $n-2$  edges to choose, $…  n-1$  vertex has $1$ edge to choose. Therefore, by multiply every edges, Icould get the running time which is $(n-1)!$. Since it is clear $(n-1)! &lt;  n!$, I can say the running time is $O(n!)$.I choose this algorithm because it is the basic way to think about how to solve this problem. This algorithm is easy to make, but it is really slow. Therefore, this algorithm makes us to realize how important the algorithm is. Better algorithm makes way more faster program.\n\nHeld-Karp Algorithm (Dynamic Algorithm)\n\nRun-time Analysis\n\n\\[O(n^{2} \\times 2^{n})\\]\n\nPseudo-code\n\n\n    \n\\begin{algorithm}\n\\caption{Held-Karp Algorithm}\n\\begin{algorithmic}\n\\Procedure{HK}{$G$, $n$}\n    \\For{$k$ \\textbf{from} $2$ \\textbf{to} $n$}\n        \\State $C(\\{k\\},k)=d_{1, k}$\n    \\EndFor\n    \\For{$s$ \\textbf{from} $2$ \\textbf{to} $n-1$}\n        \\For{$all\\ S \\subseteq \\{2,...,n\\}, |S|=s$}\n            \\For{$all\\ k \\in S$}\n                \\State $\\{C(S,k)=min_{m \\neq k, m \\in S}[C(S-\\{k\\},m)+d_{m, k}]\\}$\n            \\EndFor\n        \\EndFor\n    \\EndFor\n    \\State $opt=min_{k \\neq 1}[C(\\{2, 3, ..., n\\}, k) + d_{k, 1}]$\n    \\State \\textbf{return} $opt$\n\\EndProcedure\n\\end{algorithmic}\n\\end{algorithm}\n\n\n\n\n\nDescription\n\nThis algorithm use dynamic algorithm to solve the problem. It first gets the distance between two vertices and save it to array. It is $O(n)$ steps. The next part is to find the minimum distance with small set of vertices. The for loop, for $s$ from $2$ to $n-1$ and for all $S$  in ${2, …, n}$, $|S| =  s$, is defining small set of vertices. The next for loop is the part that getting the minimum distance from the set of vertices. Each set has s  elements and it has to compare with s  different minimum candidates, it has $O(s^{2})$ running time. The first two loop, which define the set S, has running time with $SUM_{i}=2^{n-1} (C(n-1,i))$ because there are $C(n-1, i)$ number of combination for defining set with $i$  elements, and since it is from $2$  to $n-1$, it makes $SUM_{i}=2^{n-1} (C(n-1,i)) \\times  n^{2})$. Hence, it could be rewrite as $n^{2} \\times  SUM_{i}=2^{n-1} (C(n-1,i))$ $&lt;= n^{2} \\times  2^{n-1} =  \\frac{1}{2}  \\times  n^{2} \\times  2^{n}$ $=  O(2^{n} \\times  n^{2}$). I choose this algorithm because I learned dynamic programming and this algorithm use that concepts. This algorithm saves data into $C(S, n)$ and use it to find minimum distance. By this skill, this algorithm can find real answer with way more faster than the brute force method. However, it is still slow.\n\nK-Nearest Neighbor Algorithm (Approximation Algorithm)\n\nRun-time Analysis\n\n\\[O(n^{2})\\]\n\nCode\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\ndef Distance(a,b):\n    return int(round(math.sqrt((math.pow(a['i'] - b['i'],2))+(math.pow(a['j'] - b['j'],2)))))\n\ndef KNN(cities, inFile):\n    matrix = [[-1 for x in range(len(cities))] for y in range(len(cities))]\n    minLength = sys.maxsize\n    order = []\n    for x in range(len(cities)):\n        allCities = [z for z in cities]\n        route = []\n        route.append(allCities[x]['city'])\n        allCities.remove(allCities[x])\n        length = 0\n        while len(allCities) &gt; 0:\n            current = cities[route[len(route)-1]]\n            minDistance = sys.maxsize\n            minCity = -1\n            for y in range(len(allCities)):\n                currentDistance = matrix[current['city']][allCities[y]['city']]\n                if currentDistance == -1:\n                    currentDistance = Distance(current, allCities[y])\n                    matrix[current['city']][allCities[y]['city']] = currentDistance\n                    matrix[allCities[y]['city']][current['city']] = currentDistance\n                if currentDistance &lt; minDistance:\n                    minDistance = currentDistance\n                    minCity = allCities[y]\n            route.append(minCity['city'])\n            allCities.remove(minCity)\n            length += minDistance\n        currentDistance = matrix[cities[route[0]]['city']][cities[route[len(route)-1]]['city']]\n        if currentDistance == -1:\n            currentDistance = Distance(cities[route[0]], cities[route[len(route)-1]])\n            matrix[cities[route[0]]['city']][cities[route[len(route)-1]]['city']] = currentDistance\n            matrix[cities[route[len(route)-1]]['city']][cities[route[0]]['city']] = currentDistance\n        length += currentDistance\n        if length &lt; minLength:\n            minLength = length\n            order = [x for x in route]\n            WriteFileKNN(inFile, order, minLength)\n\n\nDescription\n\nThis algorithm is greedy algorithm that finds the vertex with minimum weight and choose it. Therefore, the first vertex has to check $n-1$ edges and choose the minimum. Second one has to check n-2 edges and choose the minimum. The $n-1$ vertex choose $1$ edges. Hence, the running time is $SUM\\ 1\\ TO\\ N-1\\ = (N-1)(N-2)/2 = O(N^2)$. I choose this algorithm because I found that this is the fastest. This algorithm is greedy algorithm that choose only the nearest vertex from the start vertex, and keep finding the nearest vertex from the chosen one. Since it is just checking the nearest, there is possibility to find the wrong answer. However, I found that this algorithm finds reasonable path, which means not huge different than true answer, and the power of this algorithm is this is super fast. It is only $O(n^2)$. Therefore, this algorithm will be good choice when I need the result really quickly, and\nit is ok to have small error. I used a nearest neighbor algorithm written in Java to get inspiration for my Python program,\nfound here: http://www.sanfoundry.com/java-program-implement-traveling-salesman-problem-using-nearest-neighbour-algorithm/. The program searches for the most optimal nearest neighbor route by using a brute-force wrapper for-loop to select each city as the starting point. The program is able to analyze each\ncity for the smaller list sizes, but for the larger ones it cannot process each city within 5 minutes, so that is why I have a section of code in the main section of the program to end the algorithm after 5 minutes. Each time the algorithm finds a new shorter route it is written to the output file, so this ensures that I am still able to have a valid result at the very beginning, well before the 5 minute time constraint is over.\n\nGenetic Algorithm (Approximation Algorithm)\n\nRun-time Analysis\n\n\\[O(T \\times n_{0} \\times n^{2})\\]\n\nwhere $T$ is the number of outer iteration, $n_{0}$ is the initial size of the population, and $n$ is the number of cities.\n\nCode\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\nclass City:\n    def __init__(self, id, x, y):\n        self.id = id\n        self.x = x\n        self.y = y\n    \n    def distance(self, city):\n        # xDis = self.x - city.x\n        # yDis = self.y - city.y\n        # distance = np.sqrt((xDis ** 2) + (yDis ** 2))\n        return int(round(math.sqrt((math.pow(self.x - city.x,2))+(math.pow(self.y - city.y,2)))))\n    \n    def __repr__(self):\n        return \"(\" + str(self.x) + \",\" + str(self.y) + \")\"\n\nclass Fitness:\n    def __init__(self, route):\n        self.route = route\n        self.distance = 0\n        self.fitness= 0.0\n    \n    def routeDistance(self):\n        if self.distance ==0:\n            pathDistance = 0\n            for i in range(0, len(self.route)):\n                fromCity = self.route[i]\n                toCity = None\n                if i + 1 &lt; len(self.route):\n                    toCity = self.route[i + 1]\n                else:\n                    toCity = self.route[0]\n                pathDistance += fromCity.distance(toCity)\n            self.distance = pathDistance\n        return self.distance\n    \n    def routeFitness(self):\n        if self.fitness == 0:\n            self.fitness = 1 / float(self.routeDistance())\n        return self.fitness\ndef createRoute(cityList):\n    route = random.sample(cityList, len(cityList))\n    return route\ndef initialPopulation(popSize, cityList):\n    population = []\n\n    for i in range(0, popSize):\n        population.append(createRoute(cityList))\n    return population\ndef rankRoutes(population):\n    fitnessResults = {}\n    for i in range(0,len(population)):\n        fitnessResults[i] = Fitness(population[i]).routeFitness()\n    return sorted(fitnessResults.items(), key = operator.itemgetter(1), reverse = True)\ndef selection(popRanked, eliteSize):\n    selectionResults = []\n    df = pd.DataFrame(np.array(popRanked), columns=[\"Index\",\"Fitness\"])\n    df['cum_sum'] = df.Fitness.cumsum()\n    df['cum_perc'] = 100*df.cum_sum/df.Fitness.sum()\n    \n    for i in range(0, eliteSize):\n        selectionResults.append(popRanked[i][0])\n    for i in range(0, len(popRanked) - eliteSize):\n        pick = 100*random.random()\n        for i in range(0, len(popRanked)):\n            if pick &lt;= df.iat[i,3]:\n                selectionResults.append(popRanked[i][0])\n                break\n    return selectionResults\ndef matingPool(population, selectionResults):\n    matingpool = []\n    for i in range(0, len(selectionResults)):\n        index = selectionResults[i]\n        matingpool.append(population[index])\n    return matingpool\ndef breed(parent1, parent2):\n    child = []\n    childP1 = []\n    childP2 = []\n    \n    geneA = int(random.random() * len(parent1))\n    geneB = int(random.random() * len(parent1))\n    \n    startGene = min(geneA, geneB)\n    endGene = max(geneA, geneB)\n\n    for i in range(startGene, endGene):\n        childP1.append(parent1[i])\n        \n    childP2 = [item for item in parent2 if item not in childP1]\n\n    child = childP1 + childP2\n    return child\ndef breedPopulation(matingpool, eliteSize):\n    children = []\n    length = len(matingpool) - eliteSize\n    pool = random.sample(matingpool, len(matingpool))\n\n    for i in range(0,eliteSize):\n        children.append(matingpool[i])\n    \n    for i in range(0, length):\n        child = breed(pool[i], pool[len(matingpool)-i-1])\n        children.append(child)\n    return children\ndef mutate(individual, mutationRate):\n    for swapped in range(len(individual)):\n        if(random.random() &lt; mutationRate):\n            swapWith = int(random.random() * len(individual))\n            \n            city1 = individual[swapped]\n            city2 = individual[swapWith]\n            \n            individual[swapped] = city2\n            individual[swapWith] = city1\n    return individual\ndef mutatePopulation(population, mutationRate):\n    mutatedPop = []\n    \n    for ind in range(0, len(population)):\n        mutatedInd = mutate(population[ind], mutationRate)\n        mutatedPop.append(mutatedInd)\n    return mutatedPop\ndef nextGeneration(currentGen, eliteSize, mutationRate):\n    popRanked = rankRoutes(currentGen)\n    selectionResults = selection(popRanked, eliteSize)\n    matingpool = matingPool(currentGen, selectionResults)\n    children = breedPopulation(matingpool, eliteSize)\n    nextGeneration = mutatePopulation(children, mutationRate)\n    return nextGeneration\n\ndef GA(inFile, population, popSize, eliteSize, mutationRate, generations):\n    pop = initialPopulation(popSize, population)\n    # print(\"Initial distance: \" + str(1 / rankRoutes(pop)[0][1]))\n    \n    for i in range(0, generations):\n        pop = nextGeneration(pop, eliteSize, mutationRate)\n    \n    # print(\"Final distance: \" + str(1 / rankRoutes(pop)[0][1]))\n    bestRouteIndex = rankRoutes(pop)[0][0]\n    bestRoute = pop[bestRouteIndex]\n    WriteFileGA(inFile, bestRoute, 1 / rankRoutes(pop)[0][1])\n\n\nDescription\n\nThe genetic algorithm seems to solve different kind of problems pretty well, such as error detection, so I have thought to give it a try to solve the TSP. The implementation of the algorithm is extremely hard compared to KNN, so I need to refer to some online resource to build the algorithm on Python, the resource is found: https://towardsdatascience.com/evolution-of-a-salesman-a-complete-genetic-algorithm-tutorial-for-python-6fe5d2b3ca35. The algorithm starts with a population, a set of solutions, and every population another new population is generated to give better solutions, so as the depth of the population increases the algorithm should give better solutions in theory.\n\nTesting\n\nIn order to test that my program was actually producing valid results we used the supplied tsp-verifier.py program on each of my output routes. Each result was found to be valid.\n\nIt seems that the K-Nearest Neighbor (KNN) algorithm give better results in matter of approximation of path took and speed compared to Genetic Algorithm (GA) in all cases when time is the limitation. However, as the complexity of the graph increases and there isn't no time limits, the Genetic Algorithm can finish faster with higher approximation to the optimal path when the threshold to stop is set, compared to the KNN which takes almost 2 times longer on average to reach the same optimal path results.\n\nMy best results when testing where Test 1, 2, 4, and 5 when using KNN, and GA failed in most cases besides Test 6 and 7, which gave the same results as KNN at the same time.\n\nPlease view the next page to look into all of the results.\n\n\n  \n    \n       \n      Optimal path length\n      Predicted path length\n      Ratio\n    \n  \n  \n    \n      KNN\n      108159\n      130921\n      1.21\n    \n    \n      GA\n      108159\n      337541\n      3.12\n    \n  \n\n\n\n\n\n  \n    \n       \n      Optimal path length\n      Predicted path length\n      Ratio\n    \n  \n  \n    \n      KNN\n      2579\n      2975\n      1.15\n    \n    \n      GA\n      2579\n      27639\n      10.71\n    \n  \n\n\n\n\n\n  \n    \n       \n      Optimal path length\n      Predicted path length\n      Ratio\n    \n  \n  \n    \n      KNN\n      1573084\n      1936941\n      1.23\n    \n    \n      GA\n      1573084\n      1934200\n      1.22\n    \n  \n\n\n\n\n\n  \n    \n       \n      Predicted path length\n      Time took\n    \n  \n  \n    \n      KNN\n      5911\n      0.014127969741821289\n    \n    \n      GA\n      10488\n      14.512681007385254}\n    \n  \n\n\n\n\n\n  \n    \n       \n      Predicted path length\n      Time took\n    \n  \n  \n    \n      KNN\n      8011\n      0.11811113357543945\n    \n    \n      GA\n      32837\n      18.30189299583435\n    \n  \n\n\n\n\n\n  \n    \n       \n      Predicted path length\n      Time took\n    \n  \n  \n    \n      KNN\n      14826\n      1.3867180347442627\n    \n    \n      GA\n      103387\n      34.44625997543335\n    \n  \n\n\n\n\n\n  \n    \n       \n      Predicted path length\n      Time took\n    \n  \n  \n    \n      KNN\n      19711\n      12.897594213485718\n    \n    \n      GA\n      220727\n      73.32604813575745\n    \n  \n\n\n\n\n\n  \n    \n       \n      Predicted path length\n      Time took\n    \n  \n  \n    \n      KNN\n      27128\n      123.83275985717773\n    \n    \n      GA\n      465770\n      200.42861986160278\n    \n  \n\n\n\n\n\n  \n    \n       \n      Predicted path length\n      Time took\n    \n  \n  \n    \n      KNN\n      39834\n      299.0716700553894\n    \n    \n      GA\n      39834\n      299.00313663482666\n    \n  \n\n\n\n\n\n  \n    \n       \n      Predicted path length\n      Time took\n    \n  \n  \n    \n      KNN\n      62110\n      299.6045079231262\n    \n    \n      GA\n      62110\n      299.00823521614075"

    },
  
    {

      "title"    : "ReplaceMe",
      "url"      : "/projects/replaceme",
      "content"  : "Problem Statement\n\nI don't want to share my beautiful face on stream. However, I want to have a different representation of my face on stream in real-time that chat can change when they redeem their points on stream.\n\nI have found this interesting program, FaceRig, by Holotech Studios that allows an animated character with a face to simulate a human's face motion by mapping the human's face features to the animated character.\n\n\n\n\n  \n\n\n\nAs I want the animated character to be updated based on viewers' choice. I will need to build an API to allow the update to happen.\n\nI think there are few problems to tackle are:\n\n  Building face recognition system.\n  Compiling/finding a 3D model of an animated character.\n  Mapping the face recognition system with the 3D model.\n  Enabling the ability to update the 3D model with different animated characters from a script.\n  Building an API to allow the update from different machines on different networks.\n  Building a bot for the connectivity between the program API and Twitch API.\n\n\nPipeline\n\nI think the expected pipeline will include the following:\n\n  Computer Vision System\n    \n      Face Recognition\n      3D animated character\n    \n  \n  API\n  Web App\n  Twitch connectivity\n    \n      Twitch API\n      Nightbot\n    \n  \n\n\n\n\n    Figure 1: Expected pipeline\n\n\nNotes\n\nPlan\n\nI have found this project on Reddit by yemount. It seems that the project delivers what I am trying to accomplish in this project but using pose-estimate (detecting the entire body) and 2D image.\n\n###\n\nReferences\n\n[1]"

    },
  

  
    {

      "title"    : "Home",
      "url"      : "/notes/000-Home",
      "content"  : "Here where I will be linking everything togetherArea of Interest  [[010 Productivity]]  [[020 Knowledgebase]]  [[030 Research]]  [[040 Projects]]  [[050 School]]Highlights  [[100 Interesting Quotes]]"

    },
  
    {

      "title"    : "010 Productivity",
      "url"      : "/notes/010-Productivity",
      "content"  : "[[000 Home]]"

    },
  
    {

      "title"    : "020 Knowledgebase",
      "url"      : "/notes/020-Knowledgebase",
      "content"  : "[[000 Home]]"

    },
  
    {

      "title"    : "030 Research",
      "url"      : "/notes/030-Research",
      "content"  : "[[000 Home]]"

    },
  
    {

      "title"    : "040 Projects",
      "url"      : "/notes/040-Projects",
      "content"  : "[[000 Home]]"

    },
  
    {

      "title"    : "050 School",
      "url"      : "/notes/050-School",
      "content"  : "[[000 Home]]  [[Matrix Analysis]]  [[Natural Language Processing]]  [[Convex Optimization]]  [[Machine Learning]]  [[Digital Image Processing]]"

    },
  
    {

      "title"    : "110 Interesting Quotes",
      "url"      : "/notes/100-Interesting-Quotes",
      "content"  : "[[000 Home]]Career  [[IQ Success]]  [[IQ Programming]]"

    },
  
    {

      "title"    : "Anki",
      "url"      : "/notes/Anki",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Career",
      "url"      : "/notes/Career",
      "content"  : "[[020 Knowledgebase]]  Negotiation: [[202101140846]]"

    },
  
    {

      "title"    : "Computer Science",
      "url"      : "/notes/Computer-Science",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Computer Vision",
      "url"      : "/notes/Computer-Vision",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Data Science",
      "url"      : "/notes/Data-Science",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Data or Machine Engineering",
      "url"      : "/notes/Data-or-Machine-Engineering",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Economy",
      "url"      : "/notes/Economy",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Git",
      "url"      : "/notes/Git",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "IQ Programming",
      "url"      : "/notes/IQ-Programming",
      "content"  : "[[110 Interesting Quotes]] [[Programming]]  Dependencies (coupling) is an important concern to address, but it's only 1 of 4 criteria that I consider and it's not the most important one. I try to optimize my code around reducing state, coupling, complexity and code, in that order. I'm willing to add increased coupling if it makes my code more stateless. I'm willing to make it more complex if it reduces coupling. And I'm willing to duplicate code if it makes the code less complex. Only if it doesn't increase state, coupling or complexity do I setup code. The reason I put stateless code as the highest priority is it's the easiest to reason about. Stateless logic functions the same whether run normally, in parallel or distributed. It's the easiest to test, since it requires very little setup code. And it's the easiest to scale up, since you just run another copy of it. Once you introduce state, your life gets significantly harder. I think the reason that novice programmers optimize around code reduction is that it's the easiest of the 4 to spot. The other 3 are much more subtle and subjective and so will require greater experience to spot. But learning those priorities, in that order, has made me a significantly better developer.          \"The best programs are the ones written when the programmer is supposed to be working on something else.\" - Melinda Varian.        \"Colab or didn't happen…\" - @jesseengel  \"When people say \"I am investing for the long term\", it means they are losing money.\" - @nntaleb  \"round-robin fashion\" - Fuxin Li  \"i took this writing class in university, where an author came to talk to the class. someone asked him why he has to publish his writing, instead of just writing for himself. he was gross, and he said that writing for yourself is like masturbating and publishing is like having sex. and this has stuck with me for about 10 years.\" - http://blog.spencermounta.in/2018/how-to-be-a-blog-in-2018/index.html  \"If you torture data long enough, it will confess to anything.\" - https://www.reddit.com/r/statistics/comments/k9e43a/question_is_this_just_bad_statistics/gf3pwfi/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3  \"The will to win is not nearly as important as the will to prepare to win. Everyone wants to win, but not everyone wants to prepare to win.\" - Bobby Knight"

    },
  
    {

      "title"    : "IQ Success",
      "url"      : "/notes/IQ-Success",
      "content"  : "[[110 Interesting Quotes]] [[Success]]  This guy has gone to the zoo and interviewed all the animals. The tiger says that the secret to success is to live alone, be well disguised, have sharp claws and know how to stalk. The snail says that the secret is to live inside a solid shell, stay small, hide under dead trees and move slowly around at night. The parrot says that success lies in eating fruit, being alert, packing light, moving fast by air when necessary, and always sticking by your friends. His conclusion: These animals are giving contradictory advice! And that's because they're all \"outliers\".          But both of these points are subtly misleading. Yes, the advice is contradictory, but that's only a problem if you imagine that the animal kingdom is like a giant arena in which all the world's animals battle for the Animal Best Practices championship [1], after which all the losing animals will go extinct and the entire world will adopt the winning ways of the One True Best Animal. But, in fact, there are a hell of a lot of different ways to be a successful animal, and they coexist nicely. Indeed, they form an ecosystem in which all animals require other, much different animals to exist. https://news.ycombinator.com/item?id=469831#up_469940        \"The difference between successful people and really successful people is that really successful people say no to almost everything.\" - Warren Buffett  \"People think focus means saying yes to the thing you've got to focus on. But that's not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I'm actually as proud of the things we haven't done as the things I have done. Innovation is saying no to 1,000 things.\" - Steve Jobs  \"You've gotta keep control of your time and you can't unless you say no. You can't let people set your agenda in life.\" - Warren Buffett"

    },
  
    {

      "title"    : "Learning",
      "url"      : "/notes/Learning",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Life",
      "url"      : "/notes/Life",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Machine Learning",
      "url"      : "/notes/Machine-Learning",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Mathematics",
      "url"      : "/notes/Mathematics",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Matrix Analysis",
      "url"      : "/notes/Matrix-Analysis",
      "content"  : "[[Machine Learning]] [[Signal Processing]] [[Mathematics]]Lecture Notes  0: [[202101101808]]  1:  2:Reading Notes  Matrix Analysis  Matrix Computations  ECE 712 Course Notes"

    },
  
    {

      "title"    : "Natural Language Processing",
      "url"      : "/notes/Natural-Language-Processing",
      "content"  : "[[Machine Learning]]"

    },
  
    {

      "title"    : "Obsidian",
      "url"      : "/notes/Obsidian",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Physics",
      "url"      : "/notes/Physics",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Privacy",
      "url"      : "/notes/Privacy",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Productivity",
      "url"      : "/notes/Productivity",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "PyTorch",
      "url"      : "/notes/PyTorch",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Research",
      "url"      : "/notes/Research",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Software Engineering",
      "url"      : "/notes/Software-Engineering",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Random Notes",
      "url"      : "/notes/Startup",
      "content"  : "[[000 Home]]  Tracking your storage: [[202101140815  —]]"

    },
  
    {

      "title"    : "Statistics",
      "url"      : "/notes/Statistics",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "Writing",
      "url"      : "/notes/Writing",
      "content"  : "[[020 Knowledgebase]]"

    },
  
    {

      "title"    : "5 important features of Obsidian and how I implemented a Zettelkasten workflow!",
      "url"      : "/notes/202101081516",
      "content"  : "[[Obsidian]]  Zettelkasten method          take literature notes -&gt; removed from Obsidian      take reference notes -&gt; source note      create permanent notes      review notes        know how to move stuff from Obsidian to Anki automatically using some script or plugin          what I want is the ability to import my notes into anki      [[file^type a sentence | rename it]]"

    },
  
    {

      "title"    : "What would you do differently if you were in your latter years of college (junior-senior)",
      "url"      : "/notes/202101100831",
      "content"  : "[[Data Science]] [[Career]]  Not be content with mediocre projects but actually build and research something meaningful.  Build relationships and network for potential careers.  Internships  cozy up with a favorite professor or two  Market yourself  build projects  Be ready to make the step into career as smooth as possible.  Learn it so it sticks rather than cramming for exams  Take math and statistics classes more seriously!  Find a professor to do research with"

    },
  
    {

      "title"    : "How do you avoid answering cloze cards without first thinking about the answer ?",
      "url"      : "/notes/202101101016",
      "content"  : "[[Anki]]  problem: remembering cloze setup rather than what is being asked in the question, aka cheating  helpful tips          keep the card simple, no a lot of text, just atomic idea                  this might not work for proofs as you will need to understand what is the proof actually stating rather than memorizing the text. Hence, try to include a short answer of the proof and supplement links for the actual proof          for the proof, you can use cloze to know what is the next step of the proof (answer)                    card interval is too short, wait for longer time and it won't be easy      card numbers is too small        cloze surrounding information          ex:                  bad: Every {{c1::Tuesday}} the sky is blue but every {c1::Friday}} it is red.          good: Every {{c1::Tuesday}} the sky is {{c1::blue}} but every {{c2::Friday}} it is {{c2::red}}.                    you won't memorize the surrounding information of the needed question        reformulate the problems          before: {{c1::X}} is a symptom of disease Y      after: Disease Y has {{c1::X}} as a symptom"

    },
  
    {

      "title"    : "How to take smart notes",
      "url"      : "/notes/202101101031",
      "content"  : "[[Learning]] [[Obsidian]] [[People]]  \"[Notes] aren't a record of my thinking process. They are my thinking process.\" - [[Richard Feynman]]  \"Notes on paper, or on a computer screen […] do not make contemporary physics or other kinds of intellectual endeavor easier, they make it possible\" - [[Neil Levy]]  [[Niklas Luhmann]], the creator of Zettelkasten  \" I only do what is easy\" - [[Niklas Luhmann]]  ~ 6 notes a day  Note types:          Literature notes                  what is said          source                    Reading notes                  his actual ideas of the literature notes          what it does this mean          he writes in a way that anyone can understand it, not only himself                      Writing process          find a topic/research-question (brainstorming)      research/find literature      read and take notes      draw conclusions/outline text      write      hit or miss your deadline      start all over with the next project        The actual productivity          Thinking, connecting, and understanding      Focus on the process (not the outcome)      Writing is broken down into reasonable steps (one note at a time)                  write 3-6 notes a day                    The value of each idea compounds (more notes, better)                  connect all notes rather than storing in in an archive                    Clear distinction between permanent and temporary notes                  everything in an inbox and check whether they are required to add to your set of notes or not - [[Getting Things Done]]                      the actual learning (is - not)          spacing - cramming                  flashcards (anki), issue no context                    interleaving - one thing at a time                  baby steps, start easy and increase difficulty                    connecting - compartmentalization      self-testing - underlining and copying                  test your knowledge                    elaboration - re-reading                  when re-reading, it feels familiar so we think that it is productive. However, it isn't.          elaboration, ask questions about what you came across                          what is it about?              what does it mean for…?              how does it connect with…?              swapping perspectives: from the context of the source to the context of ones own thoughts                                  from my point-of-view, why this is important and useful?                                            does it contradict, complement, confirm or specify what I believed before?                                          creativity, it doesn't happen suddenly. It needs time and focus      further reading                  [[How to Take Smart Notes]]          [[Make it Stick]]                    \"real expertise don't make plans, they make inform decision for the given situation that they are in\" - some scientist"

    },
  
    {

      "title"    : "How genius are made: Niklas Luhmann&#39;s zettelkasten and how to be creative and productive in thinking",
      "url"      : "/notes/202101101206",
      "content"  : "[[People]] [[Zettelkasten]] [[Learning]] [[Niklas Luhmann]]  [[Niklas Luhmann]] has published more than 400 papers and 50 books, and he has more than 200 papers that were archived  his phases          reading and note-taking                  notes are sparse and condense                    Rewrite for zettelkasten (transferring the notes), every evening of the day                  what topics does the notes belong to?          combine with old notes with the same topic          then he transfer his notes into a new zettel to fit into his zettel structure of cards of the same topic                    adding and structuring zettels                  reformulating the cards          add questions and answer them          what topics belong to each others? (hub notes). He links the hub notes with an introduction to a topic                          e.g.                                  list of topics that are related, link zettel notes that introduce the topic                                                              kept an overall index of all interesting words that he will need to look into                          Arabic 45, 89, ed (introduce to the word)                                          ask questions                  he asks questions and link the zettels as a answer          he uses this to select certain questions and zettels to publish about something                      What can we do?          gather input and take notes      transfer notes to storage      frequently go through notes                  add thoughts          structure notes"

    },
  
    {

      "title"    : "Lecture 0",
      "url"      : "/notes/202101101808",
      "content"  : "[[Matrix Analysis]]Course  Structure of the course          W1: basic matrix concepts, subspace, norms      W2: [[Linear Least Squares]] (LS)      W3: [[Eigendecomposition]]                  PageRank                    W4: [[Singular Value Decomposition]]      W5 [[Positive Semidefinite Matrices]]      W6: [[Linear System of Equations]]                  other form of LS                    W7: [[Compressive Matrix Sensing]]                  new                    W8: [[Nonnegative Factorization]]                  core topic                    W9: [[Tensor Decomposition]]                  high dimension linear algebra                      Learning Resources          [[Matrix Computations by Golub]] MUST                  use as a dictionary          how to decompose matrix algorithms          for math          go-to book for matrix for Machine Learning and Signal Processing                    [[Matrix Analysis by Horn]]                  for math          explains theory                    [[ECE 712 Course Notes by Reilly]]                  for engineers          more about Signal Processing applications (detailed)                    [[Least Squares]] (LS)  Problem: given A (mxn), y (n), solve\\(\\min _{\\mathbf{x} \\in \\mathbb{R}^{n}}\\|\\mathbf{y}-\\mathbf{A} \\mathbf{x}\\|_{2}^{2}\\)  Eucildean norm:\\(\\|\\mathbf{x}\\|_{2}=\\sqrt{\\sum_{i=1}^{n}\\left|x_{i}\\right|^{2}}\\)  Assuming a tall and full-rank A, the LS solution is uniquely given by: - closed-form solution?\\(\\mathbf{x}_{\\mathrm{LS}}=\\left(\\mathbf{A}^{T} \\mathbf{A}\\right)^{-1} \\mathbf{A}^{T} \\mathbf{y}\\)Autoregression (AR) model  Application example: Linear Prediction\\(y[n]=a_{1} y[n-1]+a_{2} y[n-2]+\\cdots+a_{L} y[n-L]+w[n]\\)  $\\left{a_{i}\\right}_{i=1}^{L}$ are some coefficients  $w[n]$ is noise  Problem:\\(\\text { estimate }\\left\\{a_{i}\\right\\}_{i=1}^{L} \\text { from }\\{y[n]\\}\\)  Example: Predicting [[Hang Seng Index]]          The stock market in Hong Kong              Example: Real-time Predication of Flu          Final Project on Covid data?      ARGO, a model combining the AR model and Google search data Yang-Santillana-Kou2015      Eigenvalue Problem  Problem: given A (nxn), find v (n) such that\\(\\mathbf{A} \\mathbf{v}=\\lambda \\mathbf{v}\\)Eigendecomposition:  let A be symmetric      admits a decomposition\\[\\mathbf{A}=\\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{T}\\]    Q is orthogonal (i.e. $\\mathbf{Q}^{T} \\mathbf{Q}=\\mathbf{I}$)  $\\boldsymbol{\\Lambda}=\\operatorname{Diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n}\\right)$  no closed form in general, but can be numerically computed          what does this mean?      PageRank  Application example of Eigenvalue problem  PageRank used to rank the pages of Google search results  uses counts of links of various pages to determine pages' importanceOne-Page Explanation of How PageRank Works  Model:\\(\\sum_{j \\in \\mathcal{L}_{i}} \\frac{v_{j}}{c_{j}}=v_{i}\\)  $c_j$ is the number of outgoing links from page j  $L_i$ is set of pages with a link to page i  $v_i$ is the importance score of page i  Bryan-Tanya2006  questions          how to define it?      how to generate the solution (what if it doesn't exist?)      how to compute?      [[Low-Rank Matrix Approximation]]  Needed when reduce noise, dimension reduction (most basic useful one)  Problem: Y (mxn) is very large matrixr &lt; min(m,n), find an (A,B) in (mxr)x(rxn)\\(\\text { such that either } \\mathbf{Y}=\\mathbf{A B} \\text { or } \\mathbf{Y} \\approx \\mathbf{A B}\\)  Y, large matrices tend to have noise  AB, factorization  B, latent representation learning          what is this?        r, rank (low-rank = small as possible)  Formulation:\\(\\min _{\\mathbf{A} \\in \\mathbb{R}^{m \\times r}, \\mathbf{B} \\in \\mathbb{R}^{r \\times n}}\\|\\mathbf{Y}-\\mathbf{A B}\\|_{F}^{2}\\)[[Image Compression]]  Application of Low-rank Matrix Approximation  let Y (mxn)  store the low-rank factor pair (A,B), instead of Y          what is the relationship between rank and matrices?                  if there is no definition of rank, there isn't any difference between vectors and matrices                          what?                                            memory cost          from O(mn) to O((m+n)r)                  what is this O? It is written differently                    [[Principal Component Analysis]] (PCA)  Application of Low-rank Matrix Approximation  Problem: given {y_1, y_2, …, y_n} in (n) and k &lt; min(m,n), perform a low-dimensional representation\\(\\mathbf{y}_{i}=\\mathbf{Q} \\mathbf{c}_{i}+\\boldsymbol{\\mu}+\\mathbf{e}_{i}\\)  Q (mxk) is a basis          what is a basis?        c_i's are coefficients          what is a coefficient?        $\\mu$ is a base          what is a base?        e_i's are errors  PCA (in reduction of a face image dataset) produces a set of singular vectors (1st-400th) where each one contains a certain average of faces          so take the first 150 rank elements and discard the rest?      higher order = lower importance?      [[Singular Value Decomposition]] (SVD)  SVD: any Y (mxn) can be decomposed into\\(\\mathbf{Y}=\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{T}\\)  U (mxm) and V (nxn) are orthogonal  $\\Sigma$ (mxn) takes a diagonal form  SVD solves the low-rank matrix approximation problem[[Linear System of Equations]]  Problem: given A (nxn), y (n), solve:\\(\\mathbf{A x}=\\mathbf{y}\\)  what if your data (x) and label (y) have noise? How sensitive is your model?  Questions          How to solve it?      How to solve it when n is very large?      How sensitive is the solution x when A and y contain errors?      [[The Sparse Recovery Problem]]  Problem: given y (m), A (mxn), m &lt; n, find sparsest x (n) such that\\(\\mathbf{A x}=\\mathbf{y}\\)  sparsest = x should have as many zero elements as possibleMagnetic resonance imaging (MRI)  Application of The Sparse Recovery Problem  Problem: MRI image reconstruction  image -&gt; frequency domain using Fourier coefficients -&gt; recovery by filling unobserved Fourier coefficients to zero Cand'es-Romberg-Tao2006          unobserved Fourier coefficients?      [[Low-Rank Matrix Completion]]  Application: recommender systems  Z be a preference matrix, where $z_{ij}$ records how user i likes movie j  some $z_{ij}$ are missing since no one watches all movies  Z is assumed to be low-tank (only a few factors affect users' preferences)  Goal: guess the unkown $z_{ij}$ from the known ones  Questions:          what if every row is i.i.d, what is the rank?                  m, probability of 1                    rows and columns are collated, hence low-rank works        the winners of 2009 Netflix Grand Prize used low-rank matrix approximation Koren-Bell-Volinsky2009  Formulation (oversimplified):\\(\\min _{\\mathbf{A} \\in \\mathbb{R}^{m \\times r}, \\mathbf{B} \\in \\mathbb{R}^{r \\times n}} \\sum_{(i, j) \\in \\Omega}\\left|z_{i j}-[\\mathbf{A B}]_{i, j}\\right|^{2}\\)  $\\Omega$ is an index set that indicates the known entries of Z  can't be solved by SVD  alternating LS may be usedImage Denoising Problem  Application of Low-Rank Matrix Completion[[Nonnegative Matrix Factorization]] (NMF)  Goal: factors to be non-negative  Formulation:\\(\\min _{\\mathbf{A} \\in \\mathbb{R}^{m \\times r}, \\mathbf{B} \\in \\mathbb{R}^{r \\times n}}\\|\\mathbf{Y}-\\mathbf{A B}\\|_{F}^{2} \\quad \\text { s.t. } \\mathbf{A} \\geq \\mathbf{0}, \\mathbf{B} \\geq \\mathbf{0}\\)  able to extract meaningful features (by empirical studies)          how? And what is an empirical study?      Image Processing  basis elements extract facial features (e.g. eyes, nose, and lips) Lee-Seung1999Text Mining$Y=AB$  A is the dictionary and B is the set of weights  basis elements recover different topics  weights assign each text to its corresponding topicsFace Recognition  NMF-Extracted Features are sparse  every set correspond to a facial feature (e.g. nose, forehead)  how much weights for nose, forehead, etc.  NMF has most matrices 0s?          yes!      NMF vs. PCA in Face Recognition  NMF is sparse whereas PCA isn't, which means NMF is more effective          NMF requires more train?"

    },
  
    {

      "title"    : "Linear Least Squares",
      "url"      : "/notes/Linear-Least-Squares",
      "content"  : "[[Matrix Analysis]]"

    },
  
    {

      "title"    : "How to Take Smart Notes: Zettelkasten-Method in Notion | Simply Explained | Easy Template",
      "url"      : "/notes/202101111812",
      "content"  : "[[Zettelkasten]]      steps of Zettelkasten          Literature Notes (write in your own words)      Brief notes about the reference of the notes and some words about the total content                  what does this mean?                    Fleeting Notes                  My thoughts about the ideas presented                    Sorting the notes not-important and important notes -&gt; the important notes are the permanent notes            example          start a Literature note                  fill metadata          copy and paste logics and write literature notes about them                          does this mean paraphrasing?                                          include at the end a fleet note (what the idea is about)      if important, create a permanent note of the fleet note"

    },
  
    {

      "title"    : "Taxicab geometry",
      "url"      : "/notes/202101130921",
      "content"  : "[[Mathematics]] [[Machine Learning]]      Taxicab geometry (Manhattan dist.):          backstory: because of the grid layout of the city Manhattan. Any path that the driver take, will take the same number of intersection and length to reach to the final destination. The red, blue, and yellow lines are the possible paths to take from a point into another. Although they are all different, they both have the same total length and number of intersection, and they all share the same shortest path to the final destination!                It is called also L1-norm and LASSO in [[Machine Learning]]"

    },
  
    {

      "title"    : "As an entry-level data-analyst, do you have any room to negotiate salary?",
      "url"      : "/notes/202101140846",
      "content"  : "[[Career]]  If you have different offers, you should use it          when you state the given offers, don't give the fixed salary, but give them a range including the benefits                  Bad: I have got an offer from company X for 97k          Good: I have got an offer from company X 100-110k with benefits          to negotiate, you can state that your company offers 60-65k with package and another company offers 70-75k, but I like your company due to the future opportunities and I fit better. If we could split the difference and bump to 67.5k, I'd really conf. joining your company                    when you do the interview, you can ask to round your salary a bit more (56k-&gt;60k) and they will accept it as they won't need to go through recuritment again as it will cost more        If you don't have offers, you should do the following          Study the market for the actual price that you should get and increase a bit toward it      If you can't ask for more salary, ask for (with any reasonable reason for why you want any or all of these things)                  more benefits          more off days          a better company laptop                      If you are doing interviews with other companies          If they offered you 56k at the end of the interview, you can state that the offer is too low for the given financial obligations. Unfortunately if we aren't able to that up to at least 60k, I am going to interview with these other companies that I am recruiting with (companies hate to wait for a respond after you interview with competitors)"

    },
  
    {

      "title"    : "10 Skills to Ace Your Data Engineering Interview",
      "url"      : "/notes/202101251301",
      "content"  : "[[Data or Machine  Engineering]]  #RL: [[A proven approach to land a Data Engineering job]]  skills          SQL                  given an ERD, write queries to answer analytical questions (e.g. leetcode problems)          Select, from, where, like, joins: #RL: [[SQL Tutorial for Beginners]]          Joins: left outer, right outer, inner, full outer, anti join and know when to use a specific join type          Window functions: #RL: [[6 Key Concepts, to Master Window Functions]]          Table relationships:                          one to many, many to many, one to one type table relationships              What is primary key, foreign key                                Sub query, derived tables, CTEs          What is an index and why use it: #RL: [[What Does It Mean for a Column to Be Indexed]]                    python                  list, set, dict, tuples          Class, methods, inheritance, iterators                    Data structures and algorithms                  [[14 Patterns to Ace Any Coding Interview Question]]          Sliding window problem          Merge intervals problem          Graph BFS          Graph DFS                    Data Modeling                  given a scenario, build a schema          Data Warehousing                          [[What is a data warehouse]]              What is Star schema, fact and dimension tables: #RL: [[Kimball’s Dimensional Data Modeling]]              What are slowly changing dimensions, especially SCD1, SCD2 and SCD3 types ?: #RL: [[https://www.wikiwand.com/en/Slowly_changing_dimension]]              Separation of compute and storage: #RL: [[BigQuery under the hood]]              External tables: #RL:[[Managed vs. External Tables]]              Data partitioning: #RL: [[Hive Partitions, Types of Hive Partitioning with Examples]]              Columnar storage formats, such as parquet, orc: #RL: [[Parquet]]              OLTP vs OLAP                                OLTP:                          why use row based storage ? #RL: [[Row vs Column Oriented Databases]]                                          Data Pipelines                  design a pipeline and test it          Data pipeline dependencies, Idempotent, Time based split, late arriving events, backfilling: #RL: [[Functional Data Engineering — a modern paradigm for batch data processing]]          Basics of an orchestration tool like Airflow and DBT          Difference between ETL and ELT and when to use one over the other: #RL: [[ETL &amp; ELT, a comparison]]          CDC pattern: #RL: [[Change Data Capture Using Debezium Kafka and Pg]]          EL tools such as stitch, fivetran: #RL: [[Designing a \"low-effort\" ELT system, using stitch and dbt]]          Data Quality                    Distributed system fundamentals                  Distributed data storage and processing: #RL: [[3 Key techniques, to optimize your Apache Spark code]]          Differences between batch and stream processing                    Distributed Queue                  What is Kafka and when to use it: #RL: [[What, why, when to use Apache Kafka, with an example]]                            System Design"

    },
  
    {

      "title"    : "[D] Why you should get your PhD",
      "url"      : "/notes/202101251302",
      "content"  : "[[PhD]]  things that make PhD good:          A productive relationship with your advisor/supervisor.      learn about interesting topics without expectation of concrete output.      Day to day work which matches the skill set you want to develop      build a project based on your own ideas      The expertise of the lab and your ability to collaborate, receive feedback and socialise with them      intern with industry      Publishing your work at top tier conferences and journals        tips to select the right PhD          check if advisor's work is interesting and previous successful students worked with him      pressure to publish?      select a skill that will add additional value to your PhD after completion      narrow project or broader? Does advisor publish on different topics? Are they related? High or low quality work?      meet current lab members, know their exprtise, collabrate with them      internship during PhD is great      do lab members publish in top tier conf. regularly? What are the citations for papers?            sunk cost fallacy          When thinking about your existing projects and future projects, don’t be afraid to change tack if you worked hard on an idea and it just isn’t panning out      don’t be afraid to change supervisor and or people you collaborate with if you honestly gave it your best shot and things are not working out        you've really got to want it, and know why you're doing it."

    },
  
    {

      "title"    : "[D] Pytorch Performance guide",
      "url"      : "/notes/202101251303",
      "content"  : "[[PyTorch]]  summary          Dataloader: Use num_workers &gt; 0 and pin_memory=True      For CNNs enable cuDNN autotuner: torch.backends.cudnn.benchmark=True      Max out batch_size on your GPU      Set bias=False in Conv layers if they are followed by BatchNorm (Damn, this makes perfect sense).      Instead of model.zero_grad() use for param in model.parameters(): param.grad = None      Disable debug APIs if you don't need them      Use DistributedDataParallel instead of DataParallel      Use Apex - Don't use apex, and use the native pytorch automatic mixed precision instead - Apex is still useful for data parallel training though. - Some more under the hood stuff. #RL: [[PyTorch Performance Tuning Guide - Szymon Migacz, NVIDIA]] PYTORCH PERFORMANCETUNING GUIDE      PyTorch Lightning"

    },
  
    {

      "title"    : "[D] Is there a theoretically justified reason for choosing an optimizer for training neural networks yet in 2020?",
      "url"      : "/notes/202101251304",
      "content"  : "[[Machine Learning]]  optimization isn't used in practice, just basics are used: #RL: [[Overview of mini-batch gradient descent]], and ADAM is abused  theoretical implementation is hard compared to practice          pragmatic thought      A big issue is that very few people are actually interested in developing new theories in this field. Everyone is too focused on chasing benchmarks, on being \"pragmatic\" and on hyping up models that rely primarily on scale to succeed (eg GPT-3).        classical optimization methods are designed to solve very different problems.          NNs don't need superlinear convergence. Convergence rate is a very important property in optimization theory. You can get thousands, even millions of accurate digits in just a handful of iterations. But that doesn't matter for training NNs.        Forget theoretical justification, even empirical concerns seem to be ignored when people choose optimizers.          #RL: [[Decoupled Weight Decay Regularization]], [[New State of the Art AI Optimizer: Rectified Adam (RAdam). Improve your AI accuracy instantly versus Adam, and why it works.]], [[Lookahead Optimizer: k steps forward, 1 step back]], and [[Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks]]      comparison: #RL: [[Descending through a Crowded Valley – Benchmarking Deep Learning Optimizers]]        There's literally no theory that can justify the ridiculous performance of NNs.          In almost all other statistical domains increasing the size of the parameter space requires a quadratic increase in samples. Meanwhile Deep learning does almost the opposite. DL is stupidly effective and if we can't even understand why the solutions to these problems are so strong there's no way we can come up with a theory based method to do better.        It really just comes down to the fact that a simple optimizer like ADAM has been shown to sufficiently well at optimizing many different types of NN models across a broad range of tasks. On top of that, as a first order method parameter updates with ADAM are extremely cheap to compute compared to higher order optimization techniques. So even though you could use more complex, higher order techniques (and plenty of papers over the years have explored alternatives), ADAM trains faster and achieves nearly as good final model performance as anything else. Theory doesn't really matter for much when in practice a simpler technique performs basically as well."

    },
  
    {

      "title"    : "A practical guide for better-looking python code",
      "url"      : "/notes/202101251520",
      "content"  : "[[Software Engineering]]  #RL: [[Nine simple steps for better-looking python code]] and [[Ten Essays on Fizz Buzz]]  Do not push to the master branch          Rules for branches                  Require pull request reviews before merging          Require status checks to pass before merging (setting up CI/CD)          Include administrators                      Continuous Integration / Continuous Delivery  FizzBuzz  Pre-commit hook"

    },
  
    {

      "title"    : "Billionaires Build",
      "url"      : "/notes/202101251521",
      "content"  : "[[Career]] [[Startup]]How to be a successful YC founder:  whether what you're making will ever be something a lot of people want\\  You build something to serve one location, and then expand to others.          There have to be some people who want what you're building right now, and want it so urgently that they're willing to use it, bugs and all, even though you're a small company they've never heard of.      Who are your first users going to be, and how do you know they want this? If I had to decide whether to fund startups based on a single question, it would be \"How do you know people want this?\"      The best thing you can do in a YC interview is to teach the partners about your users. So if you want to prepare for your interview, one of the best ways to do it is to go talk to your users and find out exactly what they're thinking. Which is what you should be doing anyway.        Competitors are rarely what kills startups. Poor execution does.  The partners don't expect your idea to be perfect  in most ambitious undertakings: to be genuinely interested in what you're building. This is what really drives billionaires, or at least the ones who become billionaires from starting companies. The company is their project.  One thing few people realize about billionaires is that all of them could have stopped sooner. They could have gotten acquired, or found someone else to run the company. Many founders do. The ones who become really rich are the ones who keep working."

    },
  
    {

      "title"    : "How I use Anki as an A-level Student",
      "url"      : "/notes/202101251523",
      "content"  : "[[Anki]]  don't start big, 75-100 is good. If you want to start be, do 250 a day  yse tags instead of subdecks  use Cloze          #Q: what are those?        study by card state or tag          all review cards in random order      all cards in random order (don't reschedule)"

    },
  
    {

      "title"    : "How to learn pure mathematics on your own: a complete self-study guide",
      "url"      : "/notes/202101251524",
      "content"  : "[[Mathematics]]  Real Analysis          short paper, http://assets.press.princeton.edu/chapters/s10825.pdf      Understanding Analysis, book      lectures: https://www.youtube.com/channel/UCLzpR8AiHx9h_-yt2fAxd_A/playlists        Linear Algebra          Linear Algebra Done Right, book      lectures: https://www.youtube.com/playlist?list=PLGAnmvB9m7zOBVCZBUUmSinFV0wEir2Vw      easier problems, use this book: “Linear Algebra” by Insel, Freidberg, and Spence        Topology          book: “Topology through Inquiry” by Su and Starbird (missing)      course page: http://danaernst.com/teaching/mat441s19/materials/      free notes: http://www.math.toronto.edu/ivan/mat327/?resources      Point Set Topology Playlist: https://www.youtube.com/playlist?list=PLbMVogVj5nJRR7zYZifYopb52zjoScx1d      Algebraic Topology Playlist: https://www.youtube.com/playlist?list=PL41FDABC6AA085E78        Differential Equations          book: “Differential Equations with Boundary Value Problems” by Zill and Cullen                  Chapter 1 (Introduction!)          Chapter 4.1 (Preliminary Theory of Linear Equations)          Chapter 4.3 (Homogeneous Linear Equations with Constant Coefficients)          Chapter 7 (Laplace Transform)          Chapter 8 (Systems of Linear Differential Equations)          Chapter 9 (Numerical Methods)          Chapters 11, 12, 13 (Fourier Series and Partial Differential Equations)                      Complex Analysis          book: “Visual Complex Analysis” by Tristan Needham      Wesleyan University Playlist: https://www.youtube.com/watch?v=CVpMpZpd-5s&amp;list=PLi7yHjesblV0sSfZzWdSUXGO683n_nJdQ&amp;ab_channel=PetraBonfert-Taylor      best book for beginners: A Friendly Approach to Complex Analysis        Abstract Algebra          book: Contemporary Abstract Algebra      Socratica Abstract Algebra Playlist: https://www.youtube.com/playlist?list=PLi01XoE8jYoi3SgnnGorR_XOW3IcK-TP6      depth videos                  Group Theory: https://www.youtube.com/playlist?list=PLEAYkSg4uSQ1Yhxu2U-BxtRjZElrfVVcO          Ring and Field Theory: https://www.youtube.com/playlist?list=PLEAYkSg4uSQ3AaON5oCbS6ecwKsoopBN3                      Differential Geometry          book for intuition: “A Geometric Approach to Differential Forms” by David Bachman      book for rigor: “Introduction to Manifolds” by Loring Tu      WhyBMaths: https://www.youtube.com/watch?v=RW5lJiKZHd8&amp;list=PLxBAVPVHJPcrNrcEBKbqC_ykiVqfxZgNl&amp;ab_channel=WHYBmaths"

    },
  
    {

      "title"    : "How to Make Your Code Reviewer Fall in Love with You",
      "url"      : "/notes/202101251525",
      "content"  : "[[Software Engineering]]  Review your own code first          use diff view        Write a clear changelist description          Everything you just told me should be on the first page of your design doc      I wrote the design document imagining how my teammates would read it, but I failed to consider other readers      Your changelist description should summarize any background knowledge the reader needs      A good changelist description explains what the change achieves, at a high level, and why you’re making this change.      #RL: [[How to Write a Git Commit Message]] and [[My favourite Git commit]]        Automate the easy stuff          use CI      use pre-commit hooks        Answer questions with the code itself          You need to explain it to everyone      The best way to answer someone’s question is to refactor the code and eliminate the confusion.      Can you rename things or restructure logic to make it more clear? Code comments are an acceptable solution, but they’re strictly inferior to code that documents itself naturally.        Narrowly scope changes          Scope creep is a common anti-pattern in code reviews      do one thing      #RL: [[Curly's Law: Do One Thing]]        Separate functional and non-functional changes          The developer either fails to recognize what they did or decides that the new formatting is better. They send out a two-line functional change buried in hundreds of lines of non-functional whitespace changes.      Developers also tend to mix changes inappropriately while refactoring. I love it when my teammates refactor code, but I hate it when they refactor while changing the code’s behavior.      If a piece of code requires refactoring and behavioral changes, it should happen in two to three changelists:                  Add tests to exercise the existing behavior (if they’re not already there).          Refactor the production code while holding the test code constant.          Change behavior in the production code and update the tests to match.                    By leaving the automated tests untouched in step 2, you prove to your reviewer that your refactoring preserves behavior.      When you reach step 3, your reviewer doesn’t have to untangle the behavioral changes from the refactoring changes, as you’ve decoupled them ahead of time.        Break up large changelists          A changelist’s complexity grows exponentially with the number of code lines it touches. When my changes exceed 400 lines of production code, I look for opportunities to break it up before requesting a review.        Respond graciously to critiques          #RL: [[How to Do Code Reviews Like a Human (Part One)]] and [[The Seven Habits of Highly Effective People by Stephen R. Covey]]        Be patient when your reviewer is wrong          #RL: [[Two Ways To Design]]      refactor the code, or add comments that make the code more obviously correct      If the confusion stems from obscure language features, rewrite your code using mechanisms that are intelligible to non-experts.        Communicate your responses explicitly  Artfully solicit missing information          Whenever a reviewer gives me unclear feedback, I always respond with some variation of, “What would be helpful?”        Award all ties to your reviewer          When your reviewer makes a suggestion, and you each have roughly equal evidence to support your position, defer to your reviewer. Between the two of you, they have a better perspective on what it’s like to read this code fresh.        Minimize lag between rounds of review          Once you send your code out, driving the review to completion should be your highest priority."

    },
  
    {

      "title"    : "Linking Your Thinking",
      "url"      : "/notes/202101251526",
      "content"  : "[[Obsidian]]      Obsidian for Beginners          hotkeys:                  ctrl+e = toggle edit mode          ctrl+o = open Quick Switcher          ctrl+shift+f = search in files          ctrl+alt+ARROW = go back/forward                    #IQ: \"make notes, don't take notes\" - Nick Milo            LYT Originals                  https://www.youtube.com/watch?v=KFU72m_7-Oo&amp;list=PL3NaIVgSlAVKWUrFOkDW7AEjXaiPQu_IM&amp;index=9                              setup                          00 Home                                  top                                          title                      description                      header photo                      line                                                        content                                          index                                                  research                                                          01 interesets #MOC #evergreen                              02 Concepts #Concept #Develop                                                                                Inputs                                                          03 Sources #Source #Commentary                              04 Reference #Person #Ref                              05 Education #Class #Homework #Tutorial                                                                                Outputs                                                          06 Writing #Writing #Writing-idea                              07 Documentation #Cribsheet #Documentation #Process                              08 Journal #Log                                                                                Projects                                                          09 Compass                              10 Personal #Conversation #Context                                                                                                                                                                                      01 Interests                                  Computer and Technology                                          description                      Computer History MOC                                                  description                                                                    Computer Futurism MOC                                                  description                                                                    etc.                                                        Systems and Tools                                          description                      Tools for Thought MOC                                                  description                                                                                            etc.                                                                                                        03 Sources              same top              Tag Index                                  Source Processing                                          description                      #Source #Commentary                                                        Source Type                                          description                      #Notification #Fiction #Article #Book #Podcast #Video #Lecture #Film                                                        Reading Status                                          description                      #To-Read #Reading #Finished #To-Process #Not-Finished                                                        Naming Conventions                                          description                                                  Author First + Last Name - Name of Article (YYYY)                                                                    Barrier to Entry                                                                                                                Header embedding                          embed a section of a note that isn't mature yet                                                LYT Q&amp;As          tags = weak links                  #develop = still in progress                    LYT add to Zettlekasten                  LYT = fluid framework to manage and structure notes (middle-up and middle-bottom)          Zettlekasten = note structure (bottom-up)          folders (top-down)                    new notes that doesn't belong to MOCs yet, link to concepts MOC so you can view them on the links to be added to MOCs later on      use #Evergreen to refactor a note      doesn't add MOCs to the home note                  just keep the following                          mindsets MOC              concepts MOC              interests MOC              Writing MOC              Sources MOC              People MOC              Health MOC              Goals MOC              PKM MOC              Compass MOC              Lists MOC              Projects MOC                                setup                          notes              zettlekasten which links local notes              pull zettlekasten to MOC              MOC maps to each others              home note                                          using tags to move around                  000 Home                          0X0 YYY MOC (always case for MOC file)                                  add to the top, links: 000 Home                                            note level                                  add to the bottom, links: 0X0 YYY MOC                                                                        Progressive Summerization = the idea of highlighting what you read and summarize it into a note (aka copy-paste)                  it should be exteremly low as you aren't making notes, but taking them                    notes should be about a page long      create notes based on the idea of the article not the article              when doing a research about a project and would like to use MOC        Title        Topic 1 MOC        linklinklink        Topic 2 MOC        linklinklink        Topic 3 MOC        linklinklink            use Inbox folder to create new notes from resources or quick notes that doesn't have a base                  then every week, try to refactor the note and add it to a special MOC to be an atomic note by itself                    Using Zettlekasten system when writing notes for:                  meetings          personal tracking          trackingll          logging                    MOC system                  Inner-led Sense-making                          Mindsets              Concepts              Interests              Writings                                Outer-led Sense-making                          Sources              People                                Personal Management                          Health              Goals              Compass                                Knowledge Management                          PKM              Lists?                                Projects                          Projects                                          access note again, use a #tag      Using folders for publishing                  public          private          etc.                    TOC vs. MOC                  TOC is a finalized for publication                      MOC is more fluid"

    },
  
    {

      "title"    : "Mutlitasking As A Graduate Student",
      "url"      : "/notes/202101251527",
      "content"  : "[[Productivity]]  work in parallel by working on multiple projects at the same time  how? manage allocation of time needed to work on those projects/tasks  this will help your mind to successful accomplish multiple projects/tasks while being on track  [[Procrastination But Mitigated]] shows a method of managing those multiple projects      best approach might be to work allocate a solid unchanged timeframe for the most important thing, such as thesis work, and have multiple projects, such as homework, articles, etc. with different completion time and timeframe to be worked on in parallel.    I think the time when to work is important. Working early in the morning will result to a more clear mindset, hence, high quality work to be done  try to work on the most important things early on the morning for clearlity sake"

    },
  
    {

      "title"    : "Procrastination But Mitigated",
      "url"      : "/notes/202101251528",
      "content"  : "[[Productivity]]  Multiple tasks handling  given example          3 tasks to be completed in a week      their priorities: 6,4,2      expected of completion hours: 10, 8, 6      allocated time for daily work: 10/6, 8/4, 6/2      so I will be working on                  1: 1:45h          2: 2h          3: 3h"

    },
  
    {

      "title"    : "Skinny Pandas Riding on a Rocket (PyDataGlobal 2020)",
      "url"      : "/notes/202101251529",
      "content"  : "[[Pandas]]df_few_cols = df[['country', 'pt', 'date', 'price']]df_few_cols.info() # doesn't show the true memory, 777.2+MBdf_few.cols.info(memory_usage=\"deep\") # show true memory, 3.4GB# slimmerdf_slimmer_cols = pd.DataFrame({ 'country_cat': df_few_cols.country.astype('category'),'pt_cat': df_few_cols.pt.astype('category'),'date_cat': df_few_cols.date.astype('category'),'price_i32': df_few_cols.price.astype('int32')})%time df_few_cols.pt.value_countr() # 1.96s%time df_slimmer_cols.pt.value_countr() # 146ms  memory_profiler = profile RAM usage in python          mprof = track whole-program execution      %memit = track cost of single command"

    },
  
    {

      "title"    : "What is the best performance fix you ever achieved by changing only a few lines of code?",
      "url"      : "/notes/202101251530",
      "content"  : "[[pandas]] [[Data Science]]  This is more of a general rule: if you're using for loops in combination with Pandas, there's likely a way in pure Pandas for a performance improvement. Usually it's the first thing I go through when refactoring code.          Pandas and numpy has a lot of built-in functions that are compiled (and sometimes use SIMD etc. under the hood).      if you're implementing an algorithm or doing something complicated, it actually makes sense to keep it simple using loops and built-in python stuff and compiling it instead of trying to use numpy/pandas.      The reason is simple: compilers are better at optimizing code than you are.      The JIT compiler handles it.        if you are using SQL learn indexes, if you're using HDFS learn partitions  from ~2 min per run to ~8 seconds by, at various points in time:          swapping out indexing a pandas dataframe (which is suprisingly slow) to indexing a list of dict [code-0]      wrapping a cache decorator around some hot functions that get called redundantly      thowing some numba jit decorators around some hot matrix operations      A weird one was telling numpy to stop using so many threads, which was actually slowing down the code due to the overhead of the threads      building pandas masks up incrementally rather than doing it in the inner loop      a whole bunch of small, but effective optimizations that can be summarised as \"stop doing the same thing every time inside a for loop, instead, do it before the for loop\" - this was sometimes obsfucated by function calls      some cases of \"track and save the data now when it is cheap rather than recalculating it later on\"      #RL: [[Profiling Python using cProfile: a concrete case]]        comparing two different types, make both of them the same  Using Python’s set instead of a list for common if var in container checks. Really makes a difference if you have a bunch of items!  CPU to GPU with RAPIDS          Pandas - &gt; cudf      numpy -&gt; cupy      networkx -&gt; cugraph      [code-0]\t# before, surprisingly slow\tdf = pd.DataFrame.from_dict([{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}])\tfor idx in range(len(df)):\t\tdatum = df.iloc[idx]\t\t# do stuff\t# after, noticeably faster\tdata = [{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}]\tfor idx in range(len(df)):\t\tdatum = datum[idx]\t\t# do stuff\t# enumerate is faster\tfor idx, datum in enumerate(data):\t\t# do stuff"

    },
  
    {

      "title"    : "When attending lectures, what is your notetaking process?",
      "url"      : "/notes/202101251531",
      "content"  : "[[Studying]] [[Anki]]  Air-tun-91          method                  before lecture                          skim assigned chapter              read through chapter backwards                                  e.g. take learning objective 1 and find the answer                  take notes of the ideas                  solve simple problems                  make an anki card of it                                                              during lecture                          take notes in Q+A form                                  e.g. what is another name for the ….? ANS: …                                                              after lecture                          skim notes              make the Q+A into Anki cards                                          The lecture is supposed to reinforce the reading you already did, and walk you through the most important concepts      You should not start putting information into your spaced repetition system before you understand it      You cannot actively listen to a lecture, process the material, understand it instantly, and make a flashcard while continue to actively listen        nofunatall_17          method                  before lecture                          make an outline of the lecture in my own words              highlight key words or topics              write a few questions on what I think the important topics are                                during                          take some notes              try to get an idea of main ideas and connect them                                1st pass                          color code info highlights and underlines              fill in missing info                                2nd pass                          have notes open and make anki cards from them              double check with premade decks on the big topics covered too                                after/during review                          if there is a difficult concept to make a card for, don't understand fully, or miss it a lot on Anki, I will read up or search for more info about it              make a review sheet for it              take a screenshot of it and add it to revalvent cards"

    },
  
    {

      "title"    : "Why Obsidian Will Overtake Roam",
      "url"      : "/notes/202101251532",
      "content"  : "[[Obsidian]]  What is your PKM Personality?          Collector = Love ideas      Databaser = Love storing ideas      Writer = Love building stories      Connector = Love relating ideas        Obisdian          Databasing = linking everything                  building around the idea                    Writing mainly                  active thinking about ideas                    Connecting ideas      #IQ: \"When we read, another person thinks for us: we merely repeat his mental process… This is the case with many learned persons: they have read themselves stupid.\" - Arthur Schopenhauer        Why Obsidian is better than Roam          local files: offline, no database      file-based: every file in its own      plain text: markdown      no lock-in: you can export and play around with      free"

    },
  
    {

      "title"    : "Writing a technical book: from idea to print",
      "url"      : "/notes/202101251533",
      "content"  : "[[Writing]]  readers who knew the pattern beforehand wanted a technical manual, while conference attendees (who were being introduced to the pattern for the first time) wanted to know why the pattern matters.  come up with ideas          outline ideas      separate based on chapters        writing process          Research &amp; write code      Write the chapter while referencing the code      Revise, more research, edit code        writing style          Problem: describes the ML challenge a pattern addresses      Solution: describes one approach to solving the problem, including code snippets and recommended tooling for solving the problem      Tradeoffs and Alternatives: extended discussion on the pattern, including tools not covered in the Solution section, potential gotchas, and related solutions        review process          authors review together, each others' work      send to selected expert readers in the domain for feedback      send to publisher        burden? Ask for help to complete this with ease"

    },
  
    {

      "title"    : "Niklas Luhmann",
      "url"      : "/notes/Niklas-Luhmann",
      "content"  : "[[060 People]]"

    },
  
    {

      "title"    : "Understanding Region of Interest — (RoI Pooling)",
      "url"      : "/notes/202102272146",
      "content"  : "[[Computer Vision]]  Region of Interest (RoI) = proposal network  RoI are mapped to the model's feature map  Quantization of coordinates on the feature map          Quantization = from real numbers to discrete numbers      In case of feature maps, flooring values        RoI Pooling          RoI Pooling layer -&gt; FCL (with fixed size)      4x6 (RoI) -&gt; 3x3 (RoI pooling)      Pooling could be Max or Avg.      RoI generates proposals for the model to use in later steps. The process of RoI contains the following: 1) Selection of"

    }
  
]