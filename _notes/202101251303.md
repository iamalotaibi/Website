---
layout: post
comments: True
date: 20210125
title: "[D] Pytorch Performance guide"
source_url: https://www.reddit.com/r/MachineLearning/comments/ikr8u8/d_pytorch_performance_guide/
tags: [literature]
status: in-progress
---

[[PyTorch]]

-   summary
    -   Dataloader: Use num_workers > 0 and pin_memory=True
    -   For CNNs enable cuDNN autotuner: torch.backends.cudnn.benchmark=True
    -   Max out batch_size on your GPU
    -   Set bias=False in Conv layers if they are followed by BatchNorm (Damn, this makes perfect sense).
    -   Instead of model.zero_grad() use for param in model.parameters(): param.grad = None
    -   Disable debug APIs if you don't need them
    -   Use DistributedDataParallel instead of DataParallel
    -   Use [Apex](https://nvidia.github.io/apex/amp.html) - Don't use apex, and use the native pytorch automatic mixed precision instead - Apex is still useful for data parallel training though. - Some more under the hood stuff. #RL: [[PyTorch Performance Tuning Guide - Szymon Migacz, NVIDIA]] [PYTORCH PERFORMANCE
        TUNING GUIDE](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/szymon_migacz-pytorch-performance-tuning-guide.pdf)
    -   PyTorch Lightning
