---
layout: post
comments: True
date: 20210125
title: "[D] Is there a theoretically justified reason for choosing an optimizer for training neural networks yet in 2020?"
source_url: https://www.reddit.com/r/MachineLearning/comments/j302g8/d_is_there_a_theoretically_justified_reason_for/
tags: [literature]
status: in-progress
---

[[Machine Learning]]

-   optimization isn't used in practice, just basics are used: #RL: [[Overview of mini-batch gradient descent]], and ADAM is abused
-   theoretical implementation is hard compared to practice
    -   pragmatic thought
    -   A big issue is that very few people are actually interested in developing new theories in this field. Everyone is too focused on chasing benchmarks, on being "pragmatic" and on hyping up models that rely primarily on scale to succeed (eg GPT-3).
-   classical optimization methods are designed to solve very different problems.
    -   NNs don't need superlinear convergence. Convergence rate is a very important property in optimization theory. You can get thousands, even millions of accurate digits in just a handful of iterations. But that doesn't matter for training NNs.
-   Forget theoretical justification, even empirical concerns seem to be ignored when people choose optimizers.
    -   #RL: [[Decoupled Weight Decay Regularization]], [[New State of the Art AI Optimizer: Rectified Adam (RAdam). Improve your AI accuracy instantly versus Adam, and why it works.]], [[Lookahead Optimizer: k steps forward, 1 step back]], and [[Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks]]
    -   comparison: #RL: [[Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers]]
-   There's literally no theory that can justify the ridiculous performance of NNs.
    -   In almost all other statistical domains increasing the size of the parameter space requires a quadratic increase in samples. Meanwhile Deep learning does almost the opposite. DL is stupidly effective and if we can't even understand why the solutions to these problems are so strong there's no way we can come up with a theory based method to do better.
-   It really just comes down to the fact that a simple optimizer like ADAM has been shown to sufficiently well at optimizing many different types of NN models across a broad range of tasks. On top of that, as a first order method parameter updates with ADAM are extremely cheap to compute compared to higher order optimization techniques. So even though you could use more complex, higher order techniques (and plenty of papers over the years have explored alternatives), ADAM trains faster and achieves nearly as good final model performance as anything else. Theory doesn't really matter for much when in practice a simpler technique performs basically as well.
